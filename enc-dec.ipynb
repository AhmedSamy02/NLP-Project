{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset/\"\n",
    "CONTRACTIONS = {\n",
    "    \"n't\": \"not\",\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'d\": \"would\",\n",
    "    \"'em\": \"them\",\n",
    "    \"'all\": \"all\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"'clock\": \"oclock\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"'tween\": \"between\",\n",
    "    \"'twere\": \"it were\",\n",
    "    \"'twould\": \"it would\",\n",
    "    \"'twixt\": \"betwixt\",\n",
    "    \"'twill\": \"it will\",\n",
    "    \"'til\": \"until\",\n",
    "    \"'bout\": \"about\",\n",
    "    \"'cept\": \"except\",\n",
    "    \"'cos\": \"because\",\n",
    "    \"'fore\": \"before\",\n",
    "    \"'round\": \"around\",\n",
    "    \"'n'\": \"and\",\n",
    "    \"'neath\": \"beneath\",\n",
    "    \"'nother\": \"another\",\n",
    "    \"'nuff\": \"enough\",\n",
    "}\n",
    "negation_words = {\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "    \"never\",\n",
    "    \"without\",\n",
    "    \"avoid\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"hate\",\n",
    "    \"hold\",\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenizer = MWETokenizer() # Multi-Word Expression Tokenizer\n",
    "stop_negation_words = {\"and\", \"but\"}\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words - negation_words - stop_negation_words\n",
    "stop_words.update({\"would\", \"like\", \"get\", \"want\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w']\", \" \", text)  # Remove non-word characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = text.lower().strip()  # Lowercase and strip whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expnad_abb(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"(\" + \"|\".join(re.escape(key) for key in CONTRACTIONS.keys()) + r\")\"\n",
    "    )\n",
    "    expanded_text = pattern.sub(lambda x: \" \" + CONTRACTIONS[x.group()], text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(text):\n",
    "    # Look for patterns like \"no [word1] [word2] ...\" and transform them\n",
    "    words = text.split()\n",
    "    transformed_words = []\n",
    "    negation_flag = False  # To track if we're negating\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in negation_words:  # Trigger negation\n",
    "            negation_flag = True\n",
    "            continue  # Skip adding \"no\" to the transformed text\n",
    "        elif negation_flag and (\n",
    "            not re.match(r\"[a-zA-Z]+\", word) or word.lower() in stop_negation_words\n",
    "        ):  # End negation on punctuation or 'and'\n",
    "            negation_flag = False\n",
    "\n",
    "        # Prefix \"NOT_\" if negation flag is set\n",
    "        if negation_flag:\n",
    "            transformed_words.append(f\"NOT_{word}\")\n",
    "            if word in [\"much\"]:\n",
    "                negation_flag = False\n",
    "        else:\n",
    "            if word.lower() not in stop_negation_words:\n",
    "                transformed_words.append(word)\n",
    "\n",
    "    return \" \".join(transformed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags(sentence):\n",
    "    return f\"<s> {sentence.strip()} </s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(dataset_dir + \"PIZZA_train.json\", lines=True, nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take portion of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = df_train[\"train.SRC\"]\n",
    "output_sentences = df_train[\"train.EXR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sentences = output_sentences.apply(add_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokenizer = Tokenizer(filters=\"\")\n",
    "output_tokenizer.fit_on_texts(output_sentences)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the sequence\n",
    "\n",
    "### To make all of the same size which is the max sequence length of both input and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "max_output_length = max(len(seq) for seq in output_sequences)\n",
    "max_length = max(max_output_length, max_input_length)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding=\"post\")\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the encoder\n",
    "\n",
    "We mainly have <span style=\"color:lime\">3</span> layers\n",
    "\n",
    "1. The input layer\n",
    "2. The embedding layer to change words to numbers or vectors\n",
    "3. The LSTM layer to fit the data and generate the context vector\n",
    "\n",
    "Input --> Embeddings --> LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = Input(shape=(max_length,))\n",
    "encoder_embedding = Embedding(input_vocab_size, 128)(encoder_input)\n",
    "encoder_lstm, state_short_term, state_long_term = LSTM(256, return_state=True)(\n",
    "    encoder_embedding\n",
    ")\n",
    "encoder_context_vector = [state_short_term, state_short_term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decoder\n",
    "\n",
    "We mainly have <span style=\"color:lime\">4</span> layers\n",
    "\n",
    "1. The input layer\n",
    "2. The embedding layer to change words to numbers or vectors\n",
    "3. The LSTM layer to fit the data and generate the context vector but here we must specify the input hidden and cell is the output of the encoder [ This is the idea of encoder-decoder ]\n",
    "4. The dense layer for getting the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_input = Input(shape=(max_length - 1,))\n",
    "decoder_embedding = Embedding(output_vocab_size, 128)(decoder_input)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# No need for the hidden or the cell\n",
    "decoder_output, _, _ = decoder_lstm(\n",
    "    decoder_embedding, initial_state=encoder_context_vector\n",
    ")\n",
    "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate the full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = output_sequences[:, :-1]  # Remove the last token\n",
    "decoder_output_data = output_sequences[:, 1:]  # Remove the first token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    [input_sequences, decoder_input_data],\n",
    "    decoder_output_data,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_json(dataset_dir + \"PIZZA_dev.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_sentences = df_dev[\"dev.SRC\"]\n",
    "validation_output_sentences = df_dev[\"dev.EXR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_output, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_output = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input] + decoder_states_inputs, [decoder_output] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Ensure input sequence is padded correctly\n",
    "    input_seq = pad_sequences(\n",
    "        input_seq, maxlen=max_length, padding=\"post\"\n",
    "    )  # Padding to max_input_length\n",
    "\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index[\"<s>\"]  # Use a start token\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "        sampled_token = output_tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"</s>\":\n",
    "            stop_condition = True\n",
    "\n",
    "        # if len(decoded_sentence) > max_output_length+30:\n",
    "        #     stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "temp = validation_output_sentences.tolist()\n",
    "expected = []\n",
    "for sen in temp:\n",
    "    expected.append(sen[7:-1].strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "# Compare predictions with actual outputs\n",
    "\n",
    "for i, val_input in enumerate(validation_input_sentences):\n",
    "\n",
    "    val_seq = input_tokenizer.texts_to_sequences([val_input])\n",
    "\n",
    "    predicted_output = decode_sequence(val_seq)\n",
    "\n",
    "\n",
    "    # Tokenize expected output\n",
    "\n",
    "    reference = validation_output_sentences[i].split()\n",
    "\n",
    "    candidate = predicted_output.split()\n",
    "\n",
    "    bleu_score = sentence_bleu([reference], candidate)\n",
    "\n",
    "    print(\"Expected:\", expected[i])\n",
    "\n",
    "    print(\"Predicted:\", predicted_output)\n",
    "\n",
    "    # accuracy+=bleu_score\n",
    "    if predicted_output == expected[i]:\n",
    "        accuracy += 1\n",
    "\n",
    "print(accuracy / len(validation_input_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
