{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer, Embedding, LSTM, Dense, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "import re\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train = pd.read_json(\"dataset/PIZZA_train.json\", lines=True,)\n",
    "df_dev = pd.read_json(\"dataset/PIZZA_dev.json\", lines=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = main_train.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "x_train = df_train[\"train.SRC\"]\n",
    "y_train = df_train[\"train.EXR\"]\n",
    "\n",
    "x_dev = df_dev[\"dev.SRC\"]\n",
    "y_dev = df_dev[\"dev.EXR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w']\", \" \", text)  # Remove non-word characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = text.lower().strip()  # Lowercase and strip whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma(text):\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_14168\\1359058002.py\", line 138, in train_step\n        loss = tf.keras.losses.categorical_crossentropy(y_reshaped, y)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2199, in categorical_crossentropy\n        label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n    TypeError: Expected int32, but got 0.0 of type 'float'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 228\u001b[0m\n\u001b[0;32m    225\u001b[0m y_train_sparse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_train_padded, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use batch size 1 for this small example\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewndpnooa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 138\u001b[0m, in \u001b[0;36mPointerGeneratorNetwork.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    134\u001b[0m     y_reshaped \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(y_reshaped, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_vocab_size)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(loss)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_14168\\1359058002.py\", line 138, in train_step\n        loss = tf.keras.losses.categorical_crossentropy(y_reshaped, y)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2199, in categorical_crossentropy\n        label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n    TypeError: Expected int32, but got 0.0 of type 'float'.\n"
     ]
    }
   ],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, state_h, state_c = self.lstm(embedded)\n",
    "        return outputs, [state_h, state_c]\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.alignment_layer = Dense(units, activation='tanh')\n",
    "        self.score_layer = Dense(1)\n",
    "\n",
    "    def call(self, query, encoder_outputs, mask=None):\n",
    "        # Expand query to match encoder_outputs dimensions\n",
    "        query = tf.expand_dims(query, 1)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Repeat query to match encoder_outputs sequence length\n",
    "        query = tf.repeat(query, encoder_outputs.shape[1], axis=1)\n",
    "        \n",
    "        # Concatenate query and encoder_outputs\n",
    "        concatenated = tf.concat([query, encoder_outputs], axis=-1)\n",
    "        \n",
    "        # Compute alignment scores\n",
    "        score = self.score_layer(self.alignment_layer(concatenated))\n",
    "        score = tf.squeeze(score, axis=-1)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        if mask is not None:\n",
    "            score = tf.where(mask, score, float('-inf'))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context_vector = tf.reduce_sum(\n",
    "            attention_weights[..., tf.newaxis] * encoder_outputs, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class PointerGeneratorNetwork(Model):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 tgt_vocab_size, \n",
    "                 embedding_dim=256, \n",
    "                 hidden_dim=512, \n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(src_vocab_size, embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Embedding for decoder\n",
    "        self.embedding = Embedding(tgt_vocab_size, embedding_dim)\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output_layer = Dense(tgt_vocab_size, activation='softmax')\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Split inputs into source and target\n",
    "        source_input = inputs[:, :inputs.shape[1]//2]\n",
    "        \n",
    "        # Encoder processing\n",
    "        encoder_outputs, encoder_states = self.encoder(source_input)\n",
    "        \n",
    "        # Initial decoder state\n",
    "        decoder_states = encoder_states\n",
    "        \n",
    "        # Placeholder for decoder outputs\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        # Decoder input (use target part of input for teacher forcing)\n",
    "        decoder_input = inputs[:, inputs.shape[1]//2:]\n",
    "        \n",
    "        for t in range(self.max_length):\n",
    "            # Select current timestep input\n",
    "            current_input = decoder_input[:, t:t+1] if t < decoder_input.shape[1] else tf.fill([tf.shape(inputs)[0], 1], 0)\n",
    "            \n",
    "            # Embedding decoder input\n",
    "            decoder_embedded = self.embedding(current_input)\n",
    "            \n",
    "            # Attention\n",
    "            context_vector, _ = self.attention(\n",
    "                decoder_states[0], encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # Concatenate embedded input, context vector\n",
    "            lstm_input = tf.concat([decoder_embedded, tf.expand_dims(context_vector, 1)], axis=-1)\n",
    "            \n",
    "            # Decoder LSTM\n",
    "            decoder_output, state_h, state_c = self.decoder_lstm(\n",
    "                lstm_input, initial_state=decoder_states\n",
    "            )\n",
    "            \n",
    "            # Output layer\n",
    "            output = self.output_layer(decoder_output)  # Remove this if necessary\n",
    "            # output = tf.squeeze(output, axis=-2)  # Remove the extra dimension\n",
    "\n",
    "            \n",
    "            decoder_outputs.append(output)\n",
    "            \n",
    "            # Update decoder states\n",
    "            decoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Stack decoder outputs\n",
    "        decoder_outputs = tf.stack(decoder_outputs, axis=1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            y_pred = self(x, training=True)\n",
    "            \n",
    "            # Reshape y to match model output\n",
    "            y_reshaped = y[:, :y_pred.shape[1]]\n",
    "            y_reshaped = tf.one_hot(y_reshaped, depth=self.tgt_vocab_size)\n",
    "            \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y_reshaped, y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "# Tokenizers and data preparation\n",
    "class StructuredOutputTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.next_index = 1  # Start from 1 as 0 is typically reserved for padding\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        # Tokenize and create vocabulary\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            for token in tokens:\n",
    "                if token not in self.word_to_index:\n",
    "                    self.word_to_index[token] = self.next_index\n",
    "                    self.index_to_word[self.next_index] = token\n",
    "                    self.next_index += 1\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            sequence = [self.word_to_index.get(token, 0) for token in text.split()]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "# Prepare data\n",
    "input_tokenizer = Tokenizer(oov_token='UNK')\n",
    "output_tokenizer = StructuredOutputTokenizer()\n",
    "\n",
    "# Prepare data\n",
    "X_train = [\n",
    "    \"i'd like three large pies with pestos and yellow peppers\",\n",
    "]\n",
    "y_train = [\n",
    "    \"(ORDER (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PESTO ) (TOPPING YELLOW_PEPPERS ) ) )\",\n",
    "]\n",
    "\n",
    "# Fit tokenizers\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "output_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = input_tokenizer.texts_to_sequences(X_train)\n",
    "y_train_seq = output_tokenizer.texts_to_sequences(y_train)\n",
    "\n",
    "\n",
    "# Get vocabulary sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = output_tokenizer.next_index\n",
    "\n",
    "max_length = max(len(seq) for seq in X_train_seq)\n",
    "max_length = max(max_length, max(len(seq) for seq in y_train_seq))\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_seq,max_length, padding='post')\n",
    "y_train_padded = pad_sequences(y_train_seq, max_length,padding='post')\n",
    "\n",
    "# Combine input and output sequences\n",
    "# X_train_combined = np.concatenate([X_train_padded, y_train_padded], axis=1)\n",
    "\n",
    "# Create model\n",
    "model = PointerGeneratorNetwork(\n",
    "    src_vocab_size=input_vocab_size,\n",
    "    tgt_vocab_size=output_vocab_size\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=None  # We're using custom train_step\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare target data for training\n",
    "# Note: We need to reshape the target to match the model's output shape\n",
    "y_train_sparse = np.expand_dims(y_train_padded, axis=-1)\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    X_train_padded, \n",
    "    y_train_sparse, \n",
    "    epochs=5, \n",
    "    batch_size=1  # Use batch size 1 for this small example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "        self.lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(inputs)\n",
    "        \n",
    "        # LSTM Encoding\n",
    "        encoded_sequence, state_h, state_c = self.lstm(embedded)\n",
    "        \n",
    "        return encoded_sequence, [state_h, state_c]\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.alignment_layer = Dense(units, activation='tanh')\n",
    "        self.score_layer = Dense(1)\n",
    "\n",
    "    def call(self, query, encoder_outputs, mask=None):\n",
    "        # Expand query to match encoder_outputs dimensions\n",
    "        query = tf.expand_dims(query, 1)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Repeat query to match encoder_outputs sequence length\n",
    "        query = tf.repeat(query, encoder_outputs.shape[1], axis=1)\n",
    "        \n",
    "        # Concatenate query and encoder_outputs\n",
    "        concatenated = tf.concat([query, encoder_outputs], axis=-1)\n",
    "        \n",
    "        # Compute alignment scores\n",
    "        score = self.score_layer(self.alignment_layer(concatenated))\n",
    "        score = tf.squeeze(score, axis=-1)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        if mask is not None:\n",
    "            score = tf.where(mask, score, float('-inf'))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context_vector = tf.reduce_sum(\n",
    "            attention_weights[..., tf.newaxis] * encoder_outputs, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class PointerGeneratorNetwork(Model):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 tgt_vocab_size, \n",
    "                 embedding_dim=256, \n",
    "                 hidden_dim=512, \n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(src_vocab_size, embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Embedding for decoder\n",
    "        self.embedding = Embedding(tgt_vocab_size, embedding_dim)\n",
    "        \n",
    "        # Attention Mechanism\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "        \n",
    "        # Generation Probability Layer\n",
    "        self.generation_prob = Dense(1, activation='sigmoid')\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output_layer = Dense(tgt_vocab_size, activation='softmax')\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Encoder processing\n",
    "        encoder_outputs, encoder_states = self.encoder(inputs)\n",
    "        \n",
    "        # Initial decoder state\n",
    "        decoder_states = encoder_states\n",
    "        \n",
    "        # Placeholder for decoder outputs\n",
    "        decoder_outputs = []\n",
    "        \n",
    "        # Initial decoder input (start token)\n",
    "        decoder_input = tf.fill([tf.shape(inputs)[0], 1], 1)  # Assuming 1 is start token\n",
    "        \n",
    "        for t in range(1, self.max_length):\n",
    "            # Embedding decoder input\n",
    "            decoder_embedded = self.embedding(decoder_input)\n",
    "            \n",
    "            # Attention\n",
    "            context_vector, attention_weights = self.attention(\n",
    "                decoder_states[0], encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # Concatenate embedded input, context vector\n",
    "            lstm_input = tf.concat([decoder_embedded, tf.expand_dims(context_vector, 1)], axis=-1)\n",
    "            \n",
    "            # Decoder LSTM\n",
    "            decoder_output, state_h, state_c = self.decoder_lstm(\n",
    "                lstm_input, initial_state=decoder_states\n",
    "            )\n",
    "            \n",
    "            # Generation probability\n",
    "            p_gen = self.generation_prob(decoder_output)\n",
    "            \n",
    "            # Output layer\n",
    "            output = self.output_layer(decoder_output)\n",
    "            \n",
    "            decoder_outputs.append(output)\n",
    "            \n",
    "            # Update decoder input and states for next timestep\n",
    "            decoder_input = tf.argmax(output, axis=-1)\n",
    "            decoder_states = [state_h, state_c]\n",
    "        \n",
    "        # Stack decoder outputs\n",
    "        decoder_outputs = tf.stack(decoder_outputs, axis=1)\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            y_pred = self(x, training=True)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "def create_pointer_generator(src_vocab_size, tgt_vocab_size):\n",
    "    model = PointerGeneratorNetwork(\n",
    "        src_vocab_size=src_vocab_size, \n",
    "        tgt_vocab_size=tgt_vocab_size\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "# src_vocab_size = ...\n",
    "# tgt_vocab_size = ...\n",
    "# model = create_pointer_generator(src_vocab_size, tgt_vocab_size)\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredOutputTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_index = {\n",
    "            'PAD': 0,\n",
    "            'UNK': 1,\n",
    "            'START': 2,\n",
    "            'END': 3\n",
    "        }\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "        self.next_index = 4\n",
    "\n",
    "    def _tokenize_structured_output(self, output):\n",
    "        # Tokenize the structured output\n",
    "        tokens = []\n",
    "        current_token = ''\n",
    "        for char in output:\n",
    "            if char in ['(', ')', ' ']:\n",
    "                if current_token:\n",
    "                    # Add token if it exists\n",
    "                    if current_token not in self.token_to_index:\n",
    "                        self.token_to_index[current_token] = self.next_index\n",
    "                        self.index_to_token[self.next_index] = current_token\n",
    "                        self.next_index += 1\n",
    "                    tokens.append(current_token)\n",
    "                    current_token = ''\n",
    "                \n",
    "                if char in ['(', ')']:\n",
    "                    tokens.append(char)\n",
    "            else:\n",
    "                current_token += char\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def fit_on_texts(self, outputs):\n",
    "        for output in outputs:\n",
    "            self._tokenize_structured_output(output)\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, outputs):\n",
    "        sequences = []\n",
    "        for output in outputs:\n",
    "            tokens = self._tokenize_structured_output(output)\n",
    "            sequence = [self.token_to_index.get(token, self.token_to_index['UNK']) for token in tokens]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        return [[self.index_to_token.get(idx, 'UNK') for idx in seq] for seq in sequences]\n",
    "\n",
    "# Example usage\n",
    "input_tokenizer = Tokenizer(oov_token='UNK')\n",
    "output_tokenizer = StructuredOutputTokenizer()\n",
    "\n",
    "# Prepare data\n",
    "X_train = [\n",
    "    \"i'd like three large pies with pestos and yellow peppers\",\n",
    "]\n",
    "\n",
    "y_train = [\n",
    "    \"(ORDER (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PESTO ) (TOPPING YELLOW_PEPPERS ) ) )\",\n",
    "]\n",
    "\n",
    "# # Prepare data\n",
    "# input_tokenizer = Tokenizer(oov_token='UNK')\n",
    "# output_tokenizer = StructuredOutputTokenizer()\n",
    "\n",
    "# # Prepare data\n",
    "# X_train = [\n",
    "#     \"i'd like three large pies with pestos and yellow peppers\",\n",
    "# ]\n",
    "# y_train = [\n",
    "#     \"(ORDER (PIZZAORDER (NUMBER 3 ) (SIZE LARGE ) (TOPPING PESTO ) (TOPPING YELLOW_PEPPERS ) ) )\",\n",
    "# ]\n",
    "\n",
    "# Fit tokenizers\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "output_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = input_tokenizer.texts_to_sequences(X_train)\n",
    "y_train_seq = output_tokenizer.texts_to_sequences(y_train)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, padding='post', maxlen=30)\n",
    "y_train_padded = pad_sequences(y_train_seq, padding='post',maxlen=30)\n",
    "\n",
    "# Get vocabulary sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = output_tokenizer.next_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_14168\\3865676413.py\", line 138, in train_step\n        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 30, 1) and (1, 99, 1, 4) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m y_train_sparse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_train_padded, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use batch size 1 for this small example\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filewndpnooa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 138\u001b[0m, in \u001b[0;36mPointerGeneratorNetwork.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    135\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m    141\u001b[0m trainable_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_14168\\3865676413.py\", line 138, in train_step\n        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 30, 1) and (1, 99, 1, 4) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Modify data preparation\n",
    "# Combine source and target sequences\n",
    "# X_train_combined = np.concatenate([X_train_padded, y_train_padded], axis=1)\n",
    "\n",
    "# Create model\n",
    "model = create_pointer_generator(\n",
    "    src_vocab_size=input_vocab_size, \n",
    "    tgt_vocab_size=output_vocab_size\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Prepare target data for training\n",
    "# Note: We need to reshape the target to match the model's output shape\n",
    "y_train_sparse = np.expand_dims(y_train_padded, axis=-1)\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    X_train_padded, \n",
    "    y_train_sparse, \n",
    "    epochs=5, \n",
    "    batch_size=1  # Use batch size 1 for this small example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_19860\\1901776023.py\", line 131, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'pointer_generator_network' (type PointerGeneratorNetwork).\n    \n    in user code:\n    \n        File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_19860\\1901776023.py\", line 74, in call  *\n            source_seq, target_seq = inputs\n    \n        OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n    \n    \n    Call arguments received by layer 'pointer_generator_network' (type PointerGeneratorNetwork):\n      • inputs=tf.Tensor(shape=(None, 10), dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m create_pointer_generator(\n\u001b[0;32m      3\u001b[0m     src_vocab_size\u001b[38;5;241m=\u001b[39minput_vocab_size, \n\u001b[0;32m      4\u001b[0m     tgt_vocab_size\u001b[38;5;241m=\u001b[39moutput_vocab_size\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_19860\\1901776023.py\", line 131, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'pointer_generator_network' (type PointerGeneratorNetwork).\n    \n    in user code:\n    \n        File \"C:\\Users\\Yara\\AppData\\Local\\Temp\\ipykernel_19860\\1901776023.py\", line 74, in call  *\n            source_seq, target_seq = inputs\n    \n        OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n    \n    \n    Call arguments received by layer 'pointer_generator_network' (type PointerGeneratorNetwork):\n      • inputs=tf.Tensor(shape=(None, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_pointer_generator(\n",
    "    src_vocab_size=input_vocab_size, \n",
    "    tgt_vocab_size=output_vocab_size\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_padded, y_train_padded, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
