{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     22\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/PIZZA_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m df_train \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m200000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed Samy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ahmed Samy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed Samy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\json\\_json.py:971\u001b[0m, in \u001b[0;36mJsonReader._combine_lines\u001b[1;34m(self, lines)\u001b[0m\n\u001b[0;32m    962\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    963\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    968\u001b[0m         )\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filepath_or_buffer\n\u001b[1;32m--> 971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_combine_lines\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    972\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    Combines a list of JSON objects into one JSON object.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([line\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mline\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mline\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlines)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mline])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    977\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "df_train = pd.read_json(\"dataset/PIZZA_train.json\", lines=True,)\n",
    "df_train = df_train.sample(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 36ms/step - accuracy: 0.7914 - loss: 1.1164 - val_accuracy: 0.9215 - val_loss: 0.2847\n",
      "Epoch 2/2\n",
      "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 36ms/step - accuracy: 0.9279 - loss: 0.2592 - val_accuracy: 0.9406 - val_loss: 0.2092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x204c35ef7d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize input\n",
    "input_texts = df_train['train.SRC']\n",
    "output_texts = df_train['train.EXR']\n",
    "\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad input sequences\n",
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "\n",
    "output_tokenizer = Tokenizer(filters=\"\")\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad output sequences\n",
    "max_output_length = max(len(seq) for seq in output_sequences)\n",
    "output_sequences_padded = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "# Ensure both input and output sequences have the same length\n",
    "max_length = max(max_input_length, max_output_length)\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "output_sequences_padded = pad_sequences(output_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(max_length,))\n",
    "encoder_embedding = Embedding(input_dim=input_vocab_size, output_dim=128)(encoder_input)\n",
    "encoder_lstm, state_h, state_c = LSTM(128, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(max_length-1,))\n",
    "decoder_embedding = Embedding(input_dim=output_vocab_size, output_dim=128)(decoder_input)\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer=Adam(learning_rate=10**-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Correctly shape the decoder input and output data\n",
    "decoder_input_data = output_sequences_padded[:, :-1]  # Remove the last token\n",
    "decoder_output_data = output_sequences_padded[:, 1:]  # Remove the first token\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    [input_sequences_padded, decoder_input_data],\n",
    "    decoder_output_data,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    validation_split=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      " (pizzaorder (number 1 ) (size party_size ) (style thick_crust ) (topping american_cheese ) ) )\n"
     ]
    }
   ],
   "source": [
    "# df_dev = pd.read_json(\"dataset/PIZZA_dev.json\", lines=True)\n",
    "\n",
    "# Encoder Model\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder Model\n",
    "decoder_state_input_h = Input(shape=(128,))\n",
    "decoder_state_input_c = Input(shape=(128,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_output, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_output = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoder_output] + decoder_states\n",
    ")\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Ensure input sequence is padded correctly\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')  # Padding to max_input_length\n",
    "    \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index[\"(order\"]  # Use a start token\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "        sampled_token = output_tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        # if sampled_token == \")\" or len(decoded_sentence) > max_output_length:\n",
    "        #     stop_condition = True\n",
    "\n",
    "        # if len(decoded_sentence) > max_output_length+30:\n",
    "        #     stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "# x = df_dev['dev.SRC']\n",
    "test_input_sequence = input_tokenizer.texts_to_sequences(['i want a large onion and pepper pie with no ham'])\n",
    "output = decode_sequence(test_input_sequence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
