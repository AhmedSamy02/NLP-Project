{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,FastText\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Input,Bidirectional,Attention,Concatenate,TimeDistributed\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train = pd.read_json(\"dataset/PIZZA_train.json\", lines=True,)\n",
    "df_dev = pd.read_json(\"dataset/PIZZA_dev.json\", lines=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = main_train.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "    \"n't\": \"not\",\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'d\": \"would\",\n",
    "    \"'em\": \"them\",\n",
    "    \"'all\": \"all\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"'clock\": \"oclock\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"'tween\": \"between\",\n",
    "    \"'twere\": \"it were\",\n",
    "    \"'twould\": \"it would\",\n",
    "    \"'twixt\": \"betwixt\",\n",
    "    \"'twill\": \"it will\",\n",
    "    \"'til\": \"until\",\n",
    "    \"'bout\": \"about\",\n",
    "    \"'cept\": \"except\",\n",
    "    \"'cos\": \"because\",\n",
    "    \"'fore\": \"before\",\n",
    "    \"'round\": \"around\",\n",
    "    \"'n'\": \"and\",\n",
    "    \"'neath\": \"beneath\",\n",
    "    \"'nother\": \"another\",\n",
    "    \"'nuff\": \"enough\",\n",
    "}\n",
    "negation_words = {\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "    \"never\",\n",
    "    \"without\",\n",
    "    \"avoid\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"hate\",\n",
    "    \"hold\",\n",
    "}\n",
    "pizza = {\"pizza\", \"pizzas\", \"pie\", \"pies\"}\n",
    "\n",
    "stop_negation_words = {\"and\", \"but\"}\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words - negation_words - stop_negation_words - {'all' , 'a','an'}\n",
    "stop_words.update({\"would\", \"like\", \"get\", \"want\"})\n",
    "stop_words.update(pizza)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()  # WordNet Lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w']\", \" \", text)  # Remove non-word characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = text.lower().strip()  # Lowercase and strip whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expnad_abb2(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"(\" + \"|\".join(re.escape(key) for key in CONTRACTIONS.keys()) + r\")\"\n",
    "    )\n",
    "    expanded_text = pattern.sub(lambda x: \" \" + CONTRACTIONS[x.group()], text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(text):\n",
    "    # Look for patterns like \"no [word1] [word2] ...\" and transform them\n",
    "    words = text.split()\n",
    "    transformed_words = []\n",
    "    negation_flag = False  # To track if we're negating\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in negation_words:  # Trigger negation\n",
    "            negation_flag = True\n",
    "            continue  # Skip adding \"no\" to the transformed text\n",
    "        elif negation_flag and (\n",
    "            not re.match(r\"[a-zA-Z]+\", word) or word.lower() in stop_negation_words\n",
    "        ):  # End negation on punctuation or 'and'\n",
    "            negation_flag = False\n",
    "\n",
    "        # Prefix \"NOT_\" if negation flag is set\n",
    "        if negation_flag:\n",
    "            transformed_words.append(f\"not_{word}\")\n",
    "            if word in [\"much\"]:\n",
    "                negation_flag = False\n",
    "        else:\n",
    "            if word.lower() not in stop_negation_words:\n",
    "                transformed_words.append(word)\n",
    "\n",
    "    return \" \".join(transformed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_adj_noun(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    tokens = []\n",
    "    skip_next = False\n",
    "    \n",
    "    for i in range(len(pos_tags) - 1):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        \n",
    "        word, tag = pos_tags[i]\n",
    "        next_word, next_tag = pos_tags[i + 1]\n",
    "\n",
    "        if next_word in pizza :\n",
    "            tokens.append(word)\n",
    "            continue\n",
    "        \n",
    "        # If current word is an adjective and the next is a noun, combine them\n",
    "        if (word not in ['extra' , 'pineapple'] and tag in ['JJ', 'JJR', 'JJS'] or word == 'all') and next_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            tokens.append(f\"{word}_{next_word}\")\n",
    "            skip_next = True\n",
    "        if (word == 'seven' and next_word in ['up' , 'ups']):\n",
    "            tokens.append(f\"{word}_{next_word}\")\n",
    "            skip_next = True\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    \n",
    "    # Add the last word if it wasn't part of an adj+noun pair\n",
    "    if not skip_next:\n",
    "        tokens.append(pos_tags[-1][0])\n",
    "    \n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    text = lemma(text)\n",
    "    # text = tokenize_adj_noun(text)\n",
    "    text = expnad_abb2(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = handle_negation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pepperoni extra cheese extra ham pepperoni pepper'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize_adj_noun('i\\'d like a all veggies pizza with garlic green olive and hams')\n",
    "preprocess('pepperoni pizza with extra cheese and extra ham pepperoni pepper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['train.INPUT'] = df_train['train.SRC'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "toppings_regex = re.compile(r'(?<=\\(TOPPING\\s)[^)]*(?=\\s)')\n",
    "number_regex = re.compile(r'(?<=\\(NUMBER\\s)[^)]*(?=\\s)')\n",
    "size_regex = re.compile(r'(?<=\\(SIZE\\s)[^)]*(?=\\s)')\n",
    "quantity_regex = re.compile(r'(?<=\\(QUANTITY\\s)[^)]*(?=\\s)')\n",
    "style_regex = re.compile(r'(?<=\\(STYLE\\s)[^)]*(?=\\s)')\n",
    "drink_type_regex = re.compile(r'(?<=\\(DRINKTYPE\\s)[^)]*(?=\\s)')\n",
    "volume_regex = re.compile(r'(?<=\\(VOLUME\\s)[^)]*(?=\\s)')\n",
    "container_type_regex = re.compile(r'(?<=\\(CONTAINERTYPE\\s)[^)]*(?=\\s)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = set()\n",
    "toppings = set()\n",
    "numbers = set()\n",
    "quantities = set()\n",
    "styles = set()\n",
    "drink_types = set()\n",
    "container_types = set()\n",
    "volumes = set()\n",
    "none = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_none_match(text):\n",
    "    order_regex = re.compile(r'(?<=ORDER\\s)[^(]*(?=\\s\\()')\n",
    "    pizzaorder_regex = re.compile(r'(?<=PIZZAORDER\\s)[^(]*(?=\\s\\()')\n",
    "    drinkorder_regex = re.compile(r'(?<=DRINKORDER\\s)[^(]*(?=\\s\\()')\n",
    "    between_parentheses_regex = re.compile(r'(?<=\\)\\s)[^()]+(?=\\s\\()')\n",
    "    uncleaned_none_match = re.findall(order_regex, text)\n",
    "    uncleaned_none_match.extend(re.findall(pizzaorder_regex, text))\n",
    "    uncleaned_none_match.extend(re.findall(drinkorder_regex, text))\n",
    "    uncleaned_none_match.extend(re.findall(between_parentheses_regex, text))\n",
    "    none_match=[]\n",
    "    for sentence in uncleaned_none_match:\n",
    "        sen = expnad_abb2(sentence).upper()\n",
    "        none_match.extend([word.lower() for word in sen.split()])\n",
    "    return none_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: size_regex.findall(x)) for item in sublist])\n",
    "toppings.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: toppings_regex.findall(x)) for item in sublist])\n",
    "numbers.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: number_regex.findall(x)) for item in sublist])\n",
    "quantities.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: quantity_regex.findall(x)) for item in sublist])\n",
    "styles.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: style_regex.findall(x)) for item in sublist])\n",
    "drink_types.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: drink_type_regex.findall(x)) for item in sublist])\n",
    "container_types.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: container_type_regex.findall(x)) for item in sublist])\n",
    "volumes.update([item.lower() for sublist in df_train['train.EXR'].apply(lambda x: volume_regex.findall(x)) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: size_regex.findall(x)) for item in sublist])\n",
    "toppings.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: toppings_regex.findall(x)) for item in sublist])\n",
    "numbers.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: number_regex.findall(x)) for item in sublist])\n",
    "quantities.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: quantity_regex.findall(x)) for item in sublist])\n",
    "styles.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: style_regex.findall(x)) for item in sublist])\n",
    "drink_types.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: drink_type_regex.findall(x)) for item in sublist])\n",
    "container_types.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: container_type_regex.findall(x)) for item in sublist])\n",
    "volumes.update([item.lower() for sublist in df_train['train.TOP'].apply(lambda x: volume_regex.findall(x)) for item in sublist])\n",
    "none.update([item.lower() for sublist in df_train['train.TOP'].apply(get_none_match) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = list(sizes)\n",
    "toppings = list(toppings)\n",
    "numbers = list(numbers)\n",
    "quantities = list(quantities)\n",
    "styles = list(styles)\n",
    "drink_types = list(drink_types)\n",
    "container_types = list(container_types)\n",
    "volumes = list(volumes)\n",
    "none = list(none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_model = FastText(sentences=df_train['train.INPUT'].apply(lambda x : x.split()), vector_size=300, window=5, min_count=1, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'three': 1,\n",
       " 'four': 2,\n",
       " 'party': 3,\n",
       " 'five': 4,\n",
       " 'cheese': 5,\n",
       " 'sized': 6,\n",
       " 'pepper': 7,\n",
       " 'sprite': 8,\n",
       " 'one': 9,\n",
       " 'tea': 10,\n",
       " 'two': 11,\n",
       " 'size': 12,\n",
       " 'american': 13,\n",
       " 'ice': 14,\n",
       " 'glaze': 15,\n",
       " 'ounce': 16,\n",
       " 'balsamic': 17,\n",
       " 'large': 18,\n",
       " 'not_cheese': 19,\n",
       " 'not_american': 20,\n",
       " 'ups': 21,\n",
       " 'personal': 22,\n",
       " 'extra': 23,\n",
       " 'diet': 24,\n",
       " 'medium': 25,\n",
       " 'seven': 26,\n",
       " 'onion': 27,\n",
       " 'also': 28,\n",
       " '500': 29,\n",
       " 'sauce': 30,\n",
       " 'olive': 31,\n",
       " 'green': 32,\n",
       " 'not_pepper': 33,\n",
       " 'pecorino': 34,\n",
       " 'not_crust': 35,\n",
       " 'not_thin': 36,\n",
       " 'bottle': 37,\n",
       " 'peperonni': 38,\n",
       " 'milliliter': 39,\n",
       " 'ginger': 40,\n",
       " 'ale': 41,\n",
       " 'crust': 42,\n",
       " 'need': 43,\n",
       " 'liter': 44,\n",
       " 'fantas': 45,\n",
       " 'roasted': 46,\n",
       " 'little': 47,\n",
       " 'red': 48,\n",
       " 'lunch': 49,\n",
       " 'chicken': 50,\n",
       " 'mozzarella': 51,\n",
       " 'coke': 52,\n",
       " 'banana': 53,\n",
       " '20': 54,\n",
       " 'tomato': 55,\n",
       " 'bit': 56,\n",
       " 'lot': 57,\n",
       " 'pineapple': 58,\n",
       " 'ml': 59,\n",
       " 'pesto': 60,\n",
       " 'regular': 61,\n",
       " 'pellegrino': 62,\n",
       " 'fl': 63,\n",
       " 'iced': 64,\n",
       " 'san': 65,\n",
       " 'thin': 66,\n",
       " 'eight': 67,\n",
       " 'pellegrinos': 68,\n",
       " 'bacon': 69,\n",
       " 'pepsi': 70,\n",
       " '200': 71,\n",
       " 'fluid': 72,\n",
       " 'zero': 73,\n",
       " 'yellow': 74,\n",
       " 'pepperoni': 75,\n",
       " 'dr': 76,\n",
       " 'meatball': 77,\n",
       " 'dried': 78,\n",
       " 'not_roasted': 79,\n",
       " 'soda': 80,\n",
       " 'water': 81,\n",
       " '12': 82,\n",
       " 'buffalo': 83,\n",
       " 'lemon': 84,\n",
       " 'not_green': 85,\n",
       " 'dew': 86,\n",
       " 'mountain': 87,\n",
       " 'small': 88,\n",
       " 'caramelized': 89,\n",
       " 'not_sauce': 90,\n",
       " '8': 91,\n",
       " 'mushroom': 92,\n",
       " 'sausage': 93,\n",
       " 'barbecue': 94,\n",
       " 'garlic': 95,\n",
       " 'not_much': 96,\n",
       " '16': 97,\n",
       " 'not_red': 98,\n",
       " 'fried': 99,\n",
       " 'bbq': 100,\n",
       " 'pickle': 101,\n",
       " 'cherry': 102,\n",
       " '7': 103,\n",
       " 'doctor': 104,\n",
       " 'ham': 105,\n",
       " 'fanta': 106,\n",
       " 'not_onion': 107,\n",
       " 'jalapeno': 108,\n",
       " 'peppperoni': 109,\n",
       " 'mozarella': 110,\n",
       " 'not_many': 111,\n",
       " 'artichoke': 112,\n",
       " 'bean': 113,\n",
       " 'not_chicken': 114,\n",
       " 'anchovy': 115,\n",
       " 'kalamata': 116,\n",
       " 'beef': 117,\n",
       " 'feta': 118,\n",
       " 'not_banana': 119,\n",
       " 'tiny': 120,\n",
       " 'peper': 121,\n",
       " 'carrot': 122,\n",
       " 'not_tomato': 123,\n",
       " 'perriers': 124,\n",
       " 'black': 125,\n",
       " 'parmesan': 126,\n",
       " 'pepers': 127,\n",
       " 'vegan': 128,\n",
       " 'grilled': 129,\n",
       " 'peperronni': 130,\n",
       " 'pork': 131,\n",
       " 'pulled': 132,\n",
       " 'rise': 133,\n",
       " 'dough': 134,\n",
       " 'high': 135,\n",
       " 'hot': 136,\n",
       " 'stuffed': 137,\n",
       " 'pea': 138,\n",
       " 'shrimp': 139,\n",
       " 'arugula': 140,\n",
       " 'not_olive': 141,\n",
       " 'fat': 142,\n",
       " 'low': 143,\n",
       " 'perrier': 144,\n",
       " 'tuna': 145,\n",
       " 'balzamic': 146,\n",
       " 'oz': 147,\n",
       " 'meat': 148,\n",
       " 'oregano': 149,\n",
       " 'peperroni': 150,\n",
       " 'peperoni': 151,\n",
       " 'cheeseburger': 152,\n",
       " 'not_yellow': 153,\n",
       " 'flake': 154,\n",
       " 'ricotta': 155,\n",
       " 'white': 156,\n",
       " 'apple': 157,\n",
       " 'wood': 158,\n",
       " 'ranch': 159,\n",
       " 'sixteen': 160,\n",
       " 'chorizo': 161,\n",
       " 'not_caramelized': 162,\n",
       " 'applewood': 163,\n",
       " 'peppperonis': 164,\n",
       " 'powder': 165,\n",
       " 'alfredo': 166,\n",
       " 'chorrizo': 167,\n",
       " 'brocoli': 168,\n",
       " 'oil': 169,\n",
       " 'big': 170,\n",
       " 'ground': 171,\n",
       " 'not_bacon': 172,\n",
       " 'cumin': 173,\n",
       " 'spiced': 174,\n",
       " 'bay': 175,\n",
       " 'leaf': 176,\n",
       " 'parsley': 177,\n",
       " 'basil': 178,\n",
       " 'not_vegan': 179,\n",
       " 'not_pepperoni': 180,\n",
       " 'broccoli': 181,\n",
       " 'not_dried': 182,\n",
       " 'not_garlic': 183,\n",
       " 'cheddar': 184,\n",
       " 'lettuce': 185,\n",
       " 'italian': 186,\n",
       " 'not_buffalo': 187,\n",
       " 'meatlover': 188,\n",
       " 'thick': 189,\n",
       " 'pineaple': 190,\n",
       " 'spinach': 191,\n",
       " 'not_jalapeno': 192,\n",
       " 'spicy': 193,\n",
       " 'peperonis': 194,\n",
       " 'all': 195,\n",
       " '3': 196,\n",
       " 'rosemary': 197,\n",
       " 'not_artichoke': 198,\n",
       " 'not_low': 199,\n",
       " 'not_fat': 200,\n",
       " 'not_bbq': 201,\n",
       " 'not_ham': 202,\n",
       " 'salami': 203,\n",
       " 'not_glaze': 204,\n",
       " 'eleven': 205,\n",
       " 'gluten': 206,\n",
       " 'free': 207,\n",
       " '9': 208,\n",
       " '10': 209,\n",
       " 'ten': 210,\n",
       " '2': 211,\n",
       " 'not_sausage': 212,\n",
       " '5': 213,\n",
       " '11': 214,\n",
       " 'not_hot': 215,\n",
       " 'six': 216,\n",
       " '6': 217,\n",
       " 'lover': 218,\n",
       " 'fourteen': 219,\n",
       " 'pineaples': 220,\n",
       " 'not_pineapple': 221,\n",
       " 'not_flake': 222,\n",
       " '1': 223,\n",
       " 'not_carrot': 224,\n",
       " 'twelve': 225,\n",
       " '4': 226,\n",
       " 'nine': 227,\n",
       " 'not_barbecue': 228,\n",
       " 'new': 229,\n",
       " 'not_ricotta': 230,\n",
       " 'not_white': 231,\n",
       " '15': 232,\n",
       " 'not_feta': 233,\n",
       " '13': 234,\n",
       " 'not_ranch': 235,\n",
       " 'not_beef': 236,\n",
       " 'fifteen': 237,\n",
       " 'combination': 238,\n",
       " '14': 239,\n",
       " 'not_fried': 240,\n",
       " 'an': 241,\n",
       " 'topping': 242,\n",
       " 'not_arugula': 243,\n",
       " 'not_alfredo': 244,\n",
       " 'thirteen': 245,\n",
       " 'not_shrimp': 246,\n",
       " 'not_kalamata': 247,\n",
       " 'every': 248,\n",
       " 'style': 249,\n",
       " 'not_pulled': 250,\n",
       " 'not_pork': 251,\n",
       " 'not_balzamic': 252,\n",
       " 'not_tuna': 253,\n",
       " 'not_powder': 254,\n",
       " 'yorker': 255,\n",
       " 'not_balsamic': 256,\n",
       " 'not_cherry': 257,\n",
       " 'not_chorrizo': 258,\n",
       " 'keto': 259,\n",
       " 'not_broccoli': 260,\n",
       " 'not_rosemary': 261,\n",
       " 'not_applewood': 262,\n",
       " 'not_chorizo': 263,\n",
       " 'chicago': 264,\n",
       " 'not_apple': 265,\n",
       " 'not_lettuce': 266,\n",
       " 'veggie': 267,\n",
       " 'not_wood': 268,\n",
       " 'not_anchovy': 269,\n",
       " 'not_bay': 270,\n",
       " 'not_leaf': 271,\n",
       " 'not_grilled': 272,\n",
       " 'not_spinach': 273,\n",
       " 'cauliflower': 274,\n",
       " 'not_bean': 275,\n",
       " 'neapolitan': 276,\n",
       " 'not_italian': 277,\n",
       " 'margarita': 278,\n",
       " 'not_mozzarella': 279,\n",
       " 'everything': 280,\n",
       " 'sourdough': 281,\n",
       " 'deepdish': 282,\n",
       " 'not_cheeseburger': 283,\n",
       " 'not_brocoli': 284,\n",
       " 'not_ground': 285,\n",
       " 'not_spicy': 286,\n",
       " 'not_cheddar': 287,\n",
       " 'not_pineaple': 288,\n",
       " 'coffee': 289,\n",
       " 'meatlovers': 290,\n",
       " 'york': 291,\n",
       " 'not_spiced': 292,\n",
       " 'mediterranean': 293,\n",
       " 'not_cumin': 294,\n",
       " 'work': 295,\n",
       " 'not_pineaples': 296,\n",
       " 'not_basil': 297,\n",
       " 'margherita': 298,\n",
       " 'vegetarian': 299,\n",
       " 'napolitana': 300,\n",
       " 'vegetable': 301,\n",
       " 'pan': 302,\n",
       " 'not_mozarella': 303,\n",
       " 'mexican': 304,\n",
       " 'hawaiian': 305,\n",
       " 'dish': 306,\n",
       " 'supreme': 307,\n",
       " 'deep': 308,\n",
       " 'med': 309,\n",
       " 'napolitan': 310,\n",
       " 'not_salami': 311}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_model.wv.key_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:763\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfast_text_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfast_text_model.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\fasttext.py:615\u001b[0m, in \u001b[0;36mFastText.save\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    600\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the Fasttext model. This saved model can be loaded again using\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m    and getting vectors for out-of-vocabulary words.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    613\u001b[0m \n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1923\u001b[0m, in \u001b[0;36mWord2Vec.save\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the model.\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;124;03m    This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\u001b[39;00m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;124;03m    online training and getting vectors for vocabulary words.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \n\u001b[0;32m   1922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:766\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    764\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:606\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save the object to a file. Used internally by :meth:`gensim.utils.SaveLoad.save()`.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    602\u001b[0m \n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 606\u001b[0m restores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_specials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m     pickle(\u001b[38;5;28mself\u001b[39m, fname, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\word2vec.py:1929\u001b[0m, in \u001b[0;36mWord2Vec._save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m   1927\u001b[0m \u001b[38;5;66;03m# don't save properties that are merely calculated from others\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ignore)\u001b[38;5;241m.\u001b[39munion([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcum_table\u001b[39m\u001b[38;5;124m'\u001b[39m, ])\n\u001b[1;32m-> 1929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWord2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_specials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:670\u001b[0m, in \u001b[0;36mSaveLoad._save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    668\u001b[0m         recursive_saveloads\u001b[38;5;241m.\u001b[39mappend(attrib)\n\u001b[0;32m    669\u001b[0m         cfname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((fname, attrib))\n\u001b[1;32m--> 670\u001b[0m         restores\u001b[38;5;241m.\u001b[39mextend(\u001b[43mval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_specials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    673\u001b[0m     numpys, scipys, ignoreds \u001b[38;5;241m=\u001b[39m [], [], []\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\fasttext.py:1082\u001b[0m, in \u001b[0;36mFastTextKeyedVectors._save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# don't save properties that are merely calculated from others\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ignore)\u001b[38;5;241m.\u001b[39munion([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuckets_word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors\u001b[39m\u001b[38;5;124m'\u001b[39m, ])\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastTextKeyedVectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_specials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:682\u001b[0m, in \u001b[0;36mSaveLoad._save_specials\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[0;32m    680\u001b[0m         np\u001b[38;5;241m.\u001b[39msavez_compressed(subname(fname, attrib), val\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mascontiguousarray(val))\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 682\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrib\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsc_matrix)) \u001b[38;5;129;01mand\u001b[39;00m attrib \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore:\n\u001b[0;32m    685\u001b[0m     scipys\u001b[38;5;241m.\u001b[39mappend(attrib)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:546\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\format.py:730\u001b[0m, in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m--> 730\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[0;32m    733\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    734\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fast_text_model.save(\"fast_text_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(w1,w2,model):\n",
    "    if w1 not in model.wv or w2 not in model.wv:\n",
    "        return 0\n",
    "    A = model.wv[w1]; B = model.wv[w2]\n",
    "    return sum(A*B)/(pow(sum(pow(A,2)),0.5)*pow(sum(pow(B,2)),0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    if norm_A > 0 and norm_B > 0:\n",
    "        return dot_product / (norm_A * norm_B)\n",
    "    else:\n",
    "        return 0.0  # Return 0 if either vector is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    # 'PIZZAORDER': pizza,  # Assume `pizza` is a list of relevant entities\n",
    "    'NUMBER': numbers,    # Assume `numbers` is a list of relevant entities\n",
    "    'SIZE': sizes,        # Assume `sizes` is a list of relevant entities\n",
    "    'TOPPING': toppings,  # Assume `toppings` is a list of relevant entities\n",
    "    'STYLE': styles,      # Assume `styles` is a list of relevant entities\n",
    "    'QUANTITY': quantities,  # Assume `quantities` is a list of relevant entities\n",
    "    'DRINKTYPE': drink_types,  # Assume `drink_types` is a list of relevant entities\n",
    "    'CONTAINERTYPE': container_types,  # Assume `container_types` is a list of relevant entities\n",
    "    'VOLUME': volumes,  # Assume `volumes` is a list of relevant entities\n",
    "    'NONE': none  # Default category, assumes `none` is a list of entities or empty list\n",
    "}\n",
    "\n",
    "\n",
    "def get_best_match(token, model):\n",
    "\n",
    "    # Check if the token is in any of the relevant entities\n",
    "    for category, entity_list in categories.items():\n",
    "        \n",
    "        if token in entity_list:\n",
    "            return category\n",
    "        \n",
    "    # If no exact match, find the best match based on cosine similarity\n",
    "    best_category = None\n",
    "    best_similarity = 0.0\n",
    "\n",
    "    for category, entity_list in categories.items():\n",
    "        for entity in entity_list:\n",
    "            similarity = model.wv.similarity(token, entity)\n",
    "            # print(\"For token:\", token, \"and entity:\", entity, \"similarity:\", similarity)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                # print(\"Best similarity:\", best_similarity)\n",
    "                # print(\"Best category:\", category)\n",
    "                # print(\"Best entity:\", entity)\n",
    "                best_category = category\n",
    "\n",
    "    return best_category\n",
    "    # return best_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    s = ''\n",
    "    neg = False\n",
    "    for token in tokens:\n",
    "        if token[:3] == 'not':\n",
    "            token = token[4:]\n",
    "            neg = True\n",
    "            s+=f\"(NOT \"\n",
    "        best_macth = get_best_match(token,fast_text_model)\n",
    "        if best_macth != 'NONE' and best_macth != 'PIZZAORDER':\n",
    "            s+=f\"({best_macth} {token}) \"\n",
    "            if neg:\n",
    "                s+=f\") \"\n",
    "                neg = False \n",
    "        # elif best_macth == 'PIZZAORDER':\n",
    "        #     s = \"(PIZZAORDER) \" + s\n",
    "        # elif best_macth == 'DRINKTYPE' or best_macth == 'CONTAINERTYPE':\n",
    "        #     s+=f\"(DRINKORDER) \"\n",
    "    return s\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"two pizzas with balsamic glaze and i want two medium pizzas no american cheese\"\n",
    "input_tokens = word_tokenize(input_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(NUMBER two) (TOPPING balsamic) (TOPPING glaze) (NUMBER two) (SIZE medium) (NOT (CONTAINERTYPE american) ) (NOT (TOPPING cheese) ) '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_input(preprocess(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['train.INPUT'] = df_train['train.INPUT'].apply(lambda x: construct_input(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df_train['train.INPUT'].apply(lambda x: x.lower())\n",
    "output_texts = [\"<sos> \" + text.lower() + \" <eos>\" for text in df_train[\"train.EXR\"].tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer(filters=\"\",)  # Don't filter out any characters\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad input sequences\n",
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "\n",
    "output_tokenizer = Tokenizer(filters=\"\")\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad output sequences\n",
    "max_output_length = max(len(seq) for seq in output_sequences)\n",
    "output_sequences_padded = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "# Ensure both input and output sequences have the same length\n",
    "max_length = max(max_input_length, max_output_length)\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "output_sequences_padded = pad_sequences(output_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample mappings\n",
    "word_to_index = input_tokenizer.word_index\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_model = FastText(sentences=input_texts.apply(lambda x : x.split()), vector_size=300, window=5, min_count=1, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = embedd_model.wv.vector_size  # Dimension of Word2Vec vectors\n",
    "hidden_units = 128\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_matrix = np.zeros((input_vocab_size, embedding_dim))\n",
    "\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in embedd_model.wv.key_to_index.keys():\n",
    "        # print(\"found\" , word)\n",
    "        embedding_matrix[idx] = embedd_model.wv[word]\n",
    "    else:\n",
    "        print(\"not found\" , word)\n",
    "        embedding_matrix[idx] = np.random.uniform(-0.01, 0.01, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=input_vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    trainable=False  # Freeze embeddings if you don’t want to fine-tune them\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - accuracy: 0.7070 - loss: 1.5402 - val_accuracy: 0.8994 - val_loss: 0.3687\n",
      "Epoch 2/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 94ms/step - accuracy: 0.9098 - loss: 0.3298 - val_accuracy: 0.9263 - val_loss: 0.2603\n",
      "Epoch 3/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 96ms/step - accuracy: 0.9302 - loss: 0.2483 - val_accuracy: 0.9394 - val_loss: 0.2155\n",
      "Epoch 4/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 85ms/step - accuracy: 0.9426 - loss: 0.2054 - val_accuracy: 0.9501 - val_loss: 0.1803\n",
      "Epoch 5/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 89ms/step - accuracy: 0.9527 - loss: 0.1692 - val_accuracy: 0.9606 - val_loss: 0.1422\n"
     ]
    }
   ],
   "source": [
    "# # Tokenize input\n",
    "# input_texts = df_train['train.INPUT'].apply(lambda x : x.upper())\n",
    "# output_texts = [\"<sos> \" + text + \" <eos>\" for text in df_train[\"train.EXR\"].tolist()]  # Add start and end tokens\n",
    "\n",
    "# input_tokenizer = Tokenizer(filters=\"\")  # Don't filter out any characters\n",
    "# input_tokenizer.fit_on_texts(input_texts)\n",
    "# input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "# input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "# # Pad input sequences\n",
    "# max_input_length = max(len(seq) for seq in input_sequences)\n",
    "# input_sequences_padded = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "\n",
    "# output_tokenizer = Tokenizer(filters=\"\")\n",
    "# output_tokenizer.fit_on_texts(output_texts)\n",
    "# output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "# output_vocab_size = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# # Pad output sequences\n",
    "# max_output_length = max(len(seq) for seq in output_sequences)\n",
    "# output_sequences_padded = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "# # Ensure both input and output sequences have the same length\n",
    "# max_length = max(max_input_length, max_output_length)\n",
    "# input_sequences_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "# output_sequences_padded = pad_sequences(output_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Correctly shape the decoder input and output data\n",
    "decoder_input_data = output_sequences_padded[:, :-1]  # Remove the last token\n",
    "decoder_output_data = output_sequences_padded[:, 1:]  # Remove the first token\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=(max_length,))\n",
    "encoder_embedding = embedding_layer(encoder_input)\n",
    "encoder_lstm, state_h, state_c = LSTM(256, return_state=True, return_sequences=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(max_length-1,))\n",
    "decoder_embedding = embedding_layer(decoder_input)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention_layer = Attention()  # Using scaled dot-product attention\n",
    "context_vector = attention_layer([decoder_lstm_output, encoder_lstm])\n",
    "\n",
    "# Concatenate Context Vector and Decoder LSTM Output\n",
    "decoder_combined_context = Concatenate()([decoder_lstm_output, context_vector])\n",
    "\n",
    "# Dense Layer for Output\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_combined_context)\n",
    "\n",
    "\n",
    "# Define Model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [input_sequences_padded, decoder_input_data],\n",
    "    decoder_output_data,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Define Encoder Model\n",
    "encoder_model = Model(encoder_input, [encoder_lstm, state_h, state_c])\n",
    "\n",
    "# Define Decoder Model with Attention\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_output, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "context_vector = attention_layer([decoder_lstm_output, encoder_lstm])\n",
    "decoder_combined_context = Concatenate()([decoder_lstm_output, context_vector])\n",
    "decoder_output = decoder_dense(decoder_combined_context)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input, encoder_lstm] + decoder_states_inputs,\n",
    "    [decoder_output] + decoder_states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the encoder (using Bidirectional LSTM)\n",
    "# input_sequence = Input(shape=(None,))  # Input sequence (e.g., source sentence)\n",
    "# embedding_layer = Embedding(input_dim=input_vocab_size, output_dim=256)(input_sequence)\n",
    "# encoder = Bidirectional(LSTM(units=256, return_state=False, return_sequences=True))(embedding_layer)\n",
    "# encoder_outputs = encoder  # Shape: (batch_size, input_sequence_length, 512)\n",
    "\n",
    "# # Define the decoder (with LSTM and Attention mechanism)\n",
    "# decoder_input_sequence = Input(shape=(None,))  # Decoder input sequence (e.g., target sentence)\n",
    "# decoder_embedding = Embedding(input_dim=output_vocab_size, output_dim=256)(decoder_input_sequence)\n",
    "# decoder_lstm = LSTM(units=512, return_sequences=True, return_state=True)\n",
    "# decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=None)\n",
    "\n",
    "# # Attention mechanism: decoder output (3D tensor) and encoder outputs (3D tensor)\n",
    "# attention = Attention()([decoder_lstm_output, encoder_outputs])\n",
    "\n",
    "# # Context vector: concatenation of decoder output and attention output\n",
    "# context_vector = Concatenate()([decoder_lstm_output, attention])\n",
    "\n",
    "# # Output layer (dense layer with softmax activation)\n",
    "# output = Dense(output_vocab_size, activation='softmax')(context_vector)\n",
    "\n",
    "# # Model definition\n",
    "# model = Model([input_sequence, decoder_input_sequence], output)\n",
    "\n",
    "# # Summary of the model to check the shapes\n",
    "# model.summary()\n",
    "\n",
    "# # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# # Compile the model with categorical crossentropy and Adam optimizer\n",
    "# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# def scheduled_sampling_step(epoch, model, input_data, target_data, sampling_rate=1.0):\n",
    "#     # Initialize the decoder input with the first token (start token)\n",
    "#     decoder_input = np.zeros_like(target_data)\n",
    "#     decoder_input[:, 0] = target_data[:, 0]  # Use the first token of the ground truth as input to start\n",
    "\n",
    "#     # Prepare to store the decoder's predictions\n",
    "#     decoder_predictions = []\n",
    "\n",
    "#     # Iterate through the sequence length\n",
    "#     for t in range(1, target_data.shape[1]):  # Excluding the first token (start token)\n",
    "        \n",
    "#         # Sample a random value to decide whether to use the true token or predicted token\n",
    "#         use_true_token = np.random.rand() < sampling_rate\n",
    "        \n",
    "#         if use_true_token:\n",
    "#             # Use the true token from the target sequence\n",
    "#             decoder_input[:, t] = target_data[:, t]\n",
    "#         else:\n",
    "#             # Use the model's previous prediction as the input to the next timestep\n",
    "#             pred_token = model.predict([input_data, decoder_input])[:, t - 1, :]\n",
    "#             decoder_input[:, t] = np.argmax(pred_token, axis=-1)\n",
    "\n",
    "#         # Collect the predicted token for each timestep\n",
    "#         decoder_predictions.append(decoder_input[:, t])\n",
    "\n",
    "#     # Convert list of predictions into an array of shape (batch_size, target_length)\n",
    "#     decoder_predictions = np.array(decoder_predictions).T  # Transpose to (batch_size, target_length)\n",
    "#     return decoder_predictions\n",
    "\n",
    "\n",
    "\n",
    "# def get_batch(batch_index, batch_size, input_sequences, target_sequences, max_input_length, max_target_length):\n",
    "   \n",
    "#     # Calculate the start and end indices of the batch\n",
    "#     start_idx = batch_index * batch_size\n",
    "#     end_idx = min((batch_index + 1) * batch_size, len(input_sequences))\n",
    "    \n",
    "#     # Extract a slice of the input and target sequences\n",
    "#     input_batch = input_sequences[start_idx:end_idx]\n",
    "#     target_batch = target_sequences[start_idx:end_idx]\n",
    "    \n",
    "#     # Pad input sequences to `max_input_length`\n",
    "#     input_batch_padded = np.zeros((len(input_batch), max_input_length), dtype=np.int32)\n",
    "#     for i, seq in enumerate(input_batch):\n",
    "#         input_batch_padded[i, :len(seq)] = seq[:max_input_length]\n",
    "    \n",
    "#     # Pad target sequences to `max_target_length`\n",
    "#     target_batch_padded = np.zeros((len(target_batch), max_target_length), dtype=np.int32)\n",
    "#     for i, seq in enumerate(target_batch):\n",
    "#         target_batch_padded[i, :len(seq)] = seq[:max_target_length]\n",
    "    \n",
    "#     return input_batch_padded, target_batch_padded\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# def one_hot_encode_sequences(sequences, vocab_size):\n",
    "#     # Converts sequences of token indices into one-hot encoded format\n",
    "#     return to_categorical(sequences, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "# # Set initial sampling rate (start with 1.0, and gradually decrease it)\n",
    "# initial_sampling_rate = 1.0\n",
    "# decay_rate = 0.95  # Controls how quickly the sampling rate decays\n",
    "# min_sampling_rate = 0.2  # Minimum sampling rate to avoid dropping too early\n",
    "# batch_size = 32\n",
    "# num_batches = len(input_sequences_padded) // batch_size\n",
    "# num_epochs = 5\n",
    "\n",
    "# sampling_rate = initial_sampling_rate\n",
    "\n",
    "\n",
    "# # Training loop with scheduled sampling\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_index in range(num_batches):\n",
    "#         # Get a batch of data\n",
    "#         input_batch, target_batch = get_batch(batch_index, batch_size, input_sequences_padded, output_sequences_padded, max_input_length, max_output_length)\n",
    "        \n",
    "#         # One-hot encode the target batch\n",
    "#         target_batch_one_hot = one_hot_encode_sequences(target_batch, vocab_size=output_vocab_size)\n",
    "        \n",
    "#         # Perform scheduled sampling step to generate predicted sequence\n",
    "#         predicted_sequence = scheduled_sampling_step(epoch, model, input_batch, target_batch, sampling_rate)\n",
    "        \n",
    "#         # Trim the predicted sequence to match the target length\n",
    "#         predicted_sequence = predicted_sequence[:, :target_batch.shape[1]]  # Trim to target length\n",
    "        \n",
    "#         # Train the model on the batch using model.fit\n",
    "#         model.fit([input_batch, target_batch], target_batch_one_hot, batch_size=batch_size, epochs=1,verbose=0)\n",
    "\n",
    "#     # Update the sampling rate after each epoch\n",
    "#     sampling_rate = max(min_sampling_rate, sampling_rate * decay_rate)\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs} | Sampling rate: {sampling_rate}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Ensure input sequence is padded correctly\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "    # Get encoder output and initial states\n",
    "    encoder_output, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
    "    states_value = [state_h, state_c]\n",
    "\n",
    "    # Start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index[\"<sos>\"]  # Use a start token\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    while not stop_condition:\n",
    "        # Predict the next token with attention\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq, encoder_output] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Get the predicted token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = output_tokenizer.index_word.get(sampled_token_index, \"<unk>\")\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        # Stop if end token is generated\n",
    "        if sampled_token == \"<eos>\":\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update target sequence (input for the next timestep)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update decoder states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_sequence(input_seq):\n",
    "#     # Ensure input sequence is padded correctly\n",
    "#     input_seq = pad_sequences(input_seq, maxlen=max_input_length, padding=\"post\")\n",
    "    \n",
    "#     # Get encoder output and initial states\n",
    "#     encoder_output, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
    "#     states_value = [state_h, state_c]\n",
    "\n",
    "#     # Start token\n",
    "#     target_seq = np.zeros((1, 1))  # Shape (1, 1)\n",
    "#     target_seq[0, 0] = output_tokenizer.word_index[\"<sos>\"]  # Use start token \"<sos>\"\n",
    "    \n",
    "#     stop_condition = False\n",
    "#     decoded_sentence = \"\"\n",
    "\n",
    "#     while not stop_condition:\n",
    "#         # Predict the next token with attention\n",
    "#         output_tokens, h, c = decoder_model.predict(\n",
    "#             [target_seq, encoder_output] + states_value, verbose=0\n",
    "#         )\n",
    "\n",
    "#         # Apply a Dense layer to match the dimensionality of encoder_output\n",
    "#         # The Dense layer will project output_tokens to the same dimensionality as encoder_output (256).\n",
    "#         attention_input = Dense(256)(output_tokens)  # Project output_tokens to match the encoder's hidden size\n",
    "\n",
    "#         # Apply attention mechanism\n",
    "#         attention_output = Attention()([attention_input, encoder_output])\n",
    "#         context_vector = Concatenate()([attention_output, attention_input])\n",
    "\n",
    "#         # Get the predicted token\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])  # Get the most probable token\n",
    "#         sampled_token = output_tokenizer.index_word.get(sampled_token_index, \"<unk>\")\n",
    "#         decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "#         # Stop if end token is generated\n",
    "#         if sampled_token == \"<eos>\":\n",
    "#             stop_condition = True\n",
    "\n",
    "#         # Update target sequence (input for the next timestep)\n",
    "#         target_seq = np.zeros((1, 1))\n",
    "#         target_seq[0, 0] = sampled_token_index  # Use the predicted token as the next input\n",
    "        \n",
    "#         # Update decoder states\n",
    "#         states_value = [h, c]\n",
    "\n",
    "#     return decoded_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['dev.INPUT'] = df_dev['dev.SRC'].apply(preprocess)\n",
    "# df_dev['dev.INPUT'] = df_dev['dev.INPUT'].apply(lambda x: construct_input(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need order one large vegetarian extra banana pepper'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['dev.INPUT'].loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (STYLE VEGETARIAN ) (COMPLEX_TOPPING (QUANTITY EXTRA ) (TOPPING BANANA_PEPPERS ) ) ) )'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['dev.EXR'].loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(order (pizzaorder (number 1 ) (size large ) (style thick_crust ) (topping banana_peppers ) ) ) <eos>\n"
     ]
    }
   ],
   "source": [
    "test_input_sequence = input_tokenizer.texts_to_sequences([df_dev['dev.INPUT'].loc[test_index]])\n",
    "output = decode_sequence(test_input_sequence)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (NUMBER a) (CONTAINERTYPE bottle) (DRINKTYPE ice) (DRINKTYPE tea) (NUMBER one) (VOLUME dr) (TOPPING pepper) (NUMBER one) (NUMBER eight) (VOLUME ounce) (TOPPING pineapple) (DRINKTYPE soda) \n",
      "Expected: (ORDER (DRINKORDER (NUMBER 1 ) (VOLUME 8 OZ ) (DRINKTYPE PINEAPPLE_SODA ) ) (DRINKORDER (NUMBER 1 ) (DRINKTYPE DR_PEPPER ) ) (DRINKORDER (NUMBER 1 ) (CONTAINERTYPE BOTTLE ) (DRINKTYPE ICE_TEA ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER four) (NUMBER seven) (DRINKTYPE ups) (NUMBER three) (VOLUME dr) (DRINKTYPE pepers) (NUMBER four) (NUMBER 8) (VOLUME ounce) (TOPPING pineapple) (DRINKTYPE soda) \n",
      "Expected: (ORDER (DRINKORDER (NUMBER 4 ) (VOLUME 8 OZ ) (DRINKTYPE PINEAPPLE_SODA ) ) (DRINKORDER (NUMBER 3 ) (DRINKTYPE DR_PEPPER ) ) (DRINKORDER (NUMBER 4 ) (DRINKTYPE SEVEN_UP ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER a) (CONTAINERTYPE bottle) (DRINKTYPE ice) (DRINKTYPE tea) (NUMBER five) (VOLUME 500) (VOLUME ml) (DRINKTYPE lemon) (DRINKTYPE iced) (DRINKTYPE tea) (NUMBER a) (NUMBER eight) (VOLUME ounce) (DRINKTYPE diet) (DRINKTYPE coke) \n",
      "Expected: (ORDER (DRINKORDER (NUMBER 1 ) (VOLUME 8 OZ ) (DRINKTYPE DIET_COKE ) ) (DRINKORDER (NUMBER 5 ) (VOLUME 500 ML ) (DRINKTYPE LEMON_ICE_TEA ) ) (DRINKORDER (NUMBER 1 ) (CONTAINERTYPE BOTTLE ) (DRINKTYPE ICE_TEA ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER four) (TOPPING pesto) (NOT (TOPPING roasted) ) (NOT (TOPPING tomato) ) \n",
      "Expected: (ORDER (PIZZAORDER (NUMBER 4 ) (TOPPING PESTO ) (NOT (TOPPING ROASTED_TOMATOES ) ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER a) (SIZE small) (STYLE stuffed) (STYLE crust) (CONTAINERTYPE american) (TOPPING cheese) \n",
      "Expected: (ORDER (PIZZAORDER (NUMBER 1 ) (SIZE SMALL ) (STYLE STUFFED_CRUST ) (TOPPING AMERICAN_CHEESE ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER a) (TOPPING artichoke) (TOPPING dried) (TOPPING tomato) (TOPPING pecorino) (NOT (STYLE thin) ) (NOT (STYLE crust) ) \n",
      "Expected: (ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING ARTICHOKES ) (TOPPING DRIED_TOMATOES ) (TOPPING PECORINO_CHEESE ) (NOT (STYLE THIN_CRUST ) ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER three) (NOT (CONTAINERTYPE american) ) (NOT (TOPPING cheese) ) (NUMBER three) (SIZE party) (SIZE sized) (NUMBER a) (QUANTITY little) (TOPPING feta) (TOPPING cheese) \n",
      "Expected: (ORDER (PIZZAORDER (NUMBER 3 ) (SIZE PARTY_SIZE ) (COMPLEX_TOPPING (QUANTITY LIGHT ) (TOPPING FETA_CHEESE ) ) ) (PIZZAORDER (NUMBER 3 ) (NOT (TOPPING AMERICAN_CHEESE ) ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (TOPPING balsamic) (TOPPING glaze) (NUMBER a) (DRINKTYPE sprite) (NUMBER a) (VOLUME dr) (TOPPING pepper) (NUMBER five) (SIZE medium) (DRINKTYPE water) \n",
      "Expected: (ORDER (DRINKORDER (NUMBER 5 ) (SIZE MEDIUM ) (DRINKTYPE WATER ) ) (DRINKORDER (NUMBER 1 ) (DRINKTYPE DR_PEPPER ) ) (DRINKORDER (NUMBER 1 ) (DRINKTYPE SPRITE ) ) (PIZZAORDER (NUMBER 1 ) (TOPPING BALSAMIC_GLAZE ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER three) (SIZE party) (SIZE sized) (TOPPING olive) (TOPPING oil) (TOPPING kalamata) (TOPPING olive) \n",
      "Expected: (ORDER (PIZZAORDER (NUMBER 3 ) (SIZE PARTY_SIZE ) (TOPPING OLIVE_OIL ) (TOPPING KALAMATA_OLIVES ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n",
      "Input: (NUMBER two) (VOLUME 20) (VOLUME ounce) (DRINKTYPE diet) (DRINKTYPE sprite) (NUMBER four) (VOLUME 500) (VOLUME ml) (DRINKTYPE coke) (DRINKTYPE zero) \n",
      "Expected: (ORDER (DRINKORDER (NUMBER 4 ) (VOLUME 500 ML ) (DRINKTYPE COKE_ZERO ) ) (DRINKORDER (NUMBER 2 ) (VOLUME 20 FLOZ ) (DRINKTYPE DIET_SPRITE ) (CONTAINERTYPE CAN ) ) )\n",
      "Predicted: (order (pizzaorder (number 1 ) (topping artichokes ) (topping meatballs ) (topping ricotta_cheese ) (not (style thin_crust ) ) ) ) <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(\"Input:\", df_train['train.INPUT'].iloc[i])\n",
    "    print(\"Expected:\", df_train['train.EXR'].iloc[i])\n",
    "    test_input_sequence = input_tokenizer.texts_to_sequences([df_train['train.INPUT'].iloc[test_index].upper()])\n",
    "    output = decode_sequence(test_input_sequence)\n",
    "    print(\"Predicted:\", output)\n",
    "    print()\n",
    "\n",
    "# test_input_sequence = input_tokenizer.texts_to_sequences([df_dev['dev.INPUT'].loc[test_index].upper()])\n",
    "# output = decode_sequence(test_input_sequence)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
