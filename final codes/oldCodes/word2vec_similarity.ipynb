{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from tensorflow import keras\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(\"dataset/PIZZA_train.json\", lines=True,)\n",
    "df_train = df_train.sample(10000)\n",
    "\n",
    "df_dev = pd.read_json(\"dataset/PIZZA_dev.json\", lines=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df_train['train.SRC']:\n",
    "    for i in sent_tokenize(text):\n",
    "        temp = []\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        sentences.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# Save the trained model\n",
    "model.save(\"pizza_embeddings.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toppings_regex = re.compile(r'(?<=\\(TOPPING\\s)[^)]*(?=\\s)')\n",
    "number_regex = re.compile(r'(?<=\\(NUMBER\\s)[^)]*(?=\\s)')\n",
    "size_regex = re.compile(r'(?<=\\(SIZE\\s)[^)]*(?=\\s)')\n",
    "quantity_regex = re.compile(r'(?<=\\(QUANTITY\\s)[^)]*(?=\\s)')\n",
    "style_regex = re.compile(r'(?<=\\(STYLE\\s)[^)]*(?=\\s)')\n",
    "drink_type_regex = re.compile(r'(?<=\\(DRINKTYPE\\s)[^)]*(?=\\s)')\n",
    "container_type_regex = re.compile(r'(?<=\\(CONTAINERTYPE\\s)[^)]*(?=\\s)')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = set()\n",
    "toppings = set()\n",
    "numbers = set()\n",
    "quantities = set()\n",
    "styles = set()\n",
    "drink_types = set()\n",
    "container_types = set()\n",
    "none = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "    \"n't\": \"not\",\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'d\": \"would\",\n",
    "    \"'em\": \"them\",\n",
    "    \"'all\": \"all\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"'clock\": \"oclock\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"'tween\": \"between\",\n",
    "    \"'twere\": \"it were\",\n",
    "    \"'twould\": \"it would\",\n",
    "    \"'twixt\": \"betwixt\",\n",
    "    \"'twill\": \"it will\",\n",
    "    \"'til\": \"until\",\n",
    "    \"'bout\": \"about\",\n",
    "    \"'cept\": \"except\",\n",
    "    \"'cos\": \"because\",\n",
    "    \"'fore\": \"before\",\n",
    "    \"'round\": \"around\",\n",
    "    \"'n'\": \"and\",\n",
    "    \"'neath\": \"beneath\",\n",
    "    \"'nother\": \"another\",\n",
    "    \"'nuff\": \"enough\",\n",
    "}\n",
    "negation_words = {\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "    \"never\",\n",
    "    \"without\",\n",
    "    \"avoid\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"hate\",\n",
    "    \"hold\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expnad_abb2(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"(\" + \"|\".join(re.escape(key) for key in CONTRACTIONS.keys()) + r\")\"\n",
    "    )\n",
    "    expanded_text = pattern.sub(lambda x: \" \" + CONTRACTIONS[x.group()], text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_none_match(text):\n",
    "    order_regex = re.compile(r'(?<=ORDER\\s)[^(]*(?=\\s\\()')\n",
    "    pizzaorder_regex = re.compile(r'(?<=PIZZAORDER\\s)[^(]*(?=\\s\\()')\n",
    "    drinkorder_regex = re.compile(r'(?<=DRINKORDER\\s)[^(]*(?=\\s\\()')\n",
    "    between_parentheses_regex = re.compile(r'(?<=\\)\\s)[^()]+(?=\\s\\()')\n",
    "    uncleaned_none_match = re.findall(order_regex, text)\n",
    "    uncleaned_none_match.extend(re.findall(pizzaorder_regex, text))\n",
    "    uncleaned_none_match.extend(re.findall(drinkorder_regex, text))\n",
    "    uncleaned_none_match.extend(re.findall(between_parentheses_regex, text))\n",
    "    none_match=[]\n",
    "    for sentence in uncleaned_none_match:\n",
    "        sen = expnad_abb2(sentence).upper()\n",
    "        none_match.extend([word.lower() for word in sen.split()])\n",
    "    return none_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.update([item for sublist in df_train['train.EXR'].apply(lambda x: size_regex.findall(x)) for item in sublist])\n",
    "toppings.update([item for sublist in df_train['train.EXR'].apply(lambda x: toppings_regex.findall(x)) for item in sublist])\n",
    "numbers.update([item for sublist in df_train['train.EXR'].apply(lambda x: number_regex.findall(x)) for item in sublist])\n",
    "quantities.update([item for sublist in df_train['train.EXR'].apply(lambda x: quantity_regex.findall(x)) for item in sublist])\n",
    "styles.update([item for sublist in df_train['train.EXR'].apply(lambda x: style_regex.findall(x)) for item in sublist])\n",
    "drink_types.update([item for sublist in df_train['train.EXR'].apply(lambda x: drink_type_regex.findall(x)) for item in sublist])\n",
    "container_types.update([item for sublist in df_train['train.EXR'].apply(lambda x: container_type_regex.findall(x)) for item in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.update([item for sublist in df_train['train.TOP'].apply(lambda x: size_regex.findall(x)) for item in sublist])\n",
    "toppings.update([item for sublist in df_train['train.TOP'].apply(lambda x: toppings_regex.findall(x)) for item in sublist])\n",
    "numbers.update([item for sublist in df_train['train.TOP'].apply(lambda x: number_regex.findall(x)) for item in sublist])\n",
    "quantities.update([item for sublist in df_train['train.TOP'].apply(lambda x: quantity_regex.findall(x)) for item in sublist])\n",
    "styles.update([item for sublist in df_train['train.TOP'].apply(lambda x: style_regex.findall(x)) for item in sublist])\n",
    "drink_types.update([item for sublist in df_train['train.TOP'].apply(lambda x: drink_type_regex.findall(x)) for item in sublist])\n",
    "container_types.update([item for sublist in df_train['train.TOP'].apply(lambda x: container_type_regex.findall(x)) for item in sublist])\n",
    "none.update([item for sublist in df_train['train.TOP'].apply(get_none_match) for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = list(sizes)\n",
    "toppings = list(toppings)\n",
    "numbers = list(numbers)\n",
    "quantities = list(quantities)\n",
    "styles = list(styles)\n",
    "drink_types = list(drink_types)\n",
    "container_types = list(container_types)\n",
    "none = list(none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I would like three large pies with pesto and yellow peppers\"\n",
    "input_tokens = word_tokenize(input_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(w1,w2,model):\n",
    "    if w1 not in model.wv or w2 not in model.wv:\n",
    "        return 0\n",
    "    A = model.wv[w1]; B = model.wv[w2]\n",
    "    return sum(A*B)/(pow(sum(pow(A,2)),0.5)*pow(sum(pow(B,2)),0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for token  i We got ('none', 1.0)\n",
      "for token  would We got ('none', 0)\n",
      "for token  like We got ('none', 0.9999999999999999)\n",
      "for token  three We got ('number', 0.9999999999999999)\n",
      "for token  large We got ('size', 0.9999999999999998)\n",
      "for token  pies We got ('none', 1.0000000000000002)\n",
      "for token  with We got ('none', 1.0)\n",
      "for token  pesto We got ('toppings', 1.0000000000000002)\n",
      "for token  and We got ('none', 1.0)\n",
      "for token  yellow We got ('toppings', 0.868466221107743)\n",
      "for token  peppers We got ('toppings', 0.9999999999999999)\n"
     ]
    }
   ],
   "source": [
    "categories = {\n",
    "    # 'PIZZAORDER': pizza,  # Assume `pizza` is a list of relevant entities\n",
    "    'NUMBER': numbers,    # Assume `numbers` is a list of relevant entities\n",
    "    'SIZE': sizes,        # Assume `sizes` is a list of relevant entities\n",
    "    'TOPPING': toppings,  # Assume `toppings` is a list of relevant entities\n",
    "    'STYLE': styles,      # Assume `styles` is a list of relevant entities\n",
    "    'QUANTITY': quantities,  # Assume `quantities` is a list of relevant entities\n",
    "    'DRINKTYPE': drink_types,  # Assume `drink_types` is a list of relevant entities\n",
    "    'CONTAINERTYPE': container_types,  # Assume `container_types` is a list of relevant entities\n",
    "    'NONE': none  # Default category, assumes `none` is a list of entities or empty list\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_best_match(token, model):\n",
    "\n",
    "    # Check if the token is in any of the relevant entities\n",
    "    for category, entity_list in categories.items():\n",
    "        \n",
    "        if token in entity_list:\n",
    "            return category\n",
    "        \n",
    "    # If no exact match, find the best match based on cosine similarity\n",
    "    best_category = None\n",
    "    best_similarity = 0.0\n",
    "\n",
    "    for category, entity_list in categories.items():\n",
    "        for entity in entity_list:\n",
    "            similarity = model.wv.similarity(token, entity)\n",
    "            # print(\"For token:\", token, \"and entity:\", entity, \"similarity:\", similarity)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                # print(\"Best similarity:\", best_similarity)\n",
    "                # print(\"Best category:\", category)\n",
    "                # print(\"Best entity:\", entity)\n",
    "                best_category = category\n",
    "\n",
    "    return best_category\n",
    "    # return best_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_order_entry = {\n",
    "        \"NUMBER\": None,\n",
    "        \"SIZE\": None,\n",
    "        \"STYLE\": None,\n",
    "        \"AllTopping\": []\n",
    "    }\n",
    "\n",
    "drink_order_entry = {\n",
    "        \"NUMBER\": None,\n",
    "        \"SIZE\": None,\n",
    "        \"DRINKTYPE\": None,\n",
    "        \"CONTAINERTYPE\": None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ORDER\": {\n",
      "        \"PIZZAORDER\": [\n",
      "            {\n",
      "                \"NUMBER\": \"a\",\n",
      "                \"SIZE\": \"small\",\n",
      "                \"STYLE\": \"'d\",\n",
      "                \"AllTopping\": [\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"pineapple\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"buffalo\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"chicken\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"garlic\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"NOT\": false,\n",
      "                        \"Quantity\": null,\n",
      "                        \"Topping\": \"powder\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"DRINKORDER\": [\n",
      "            {}\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def process_toppings(toppings_list, model):\n",
    "    all_toppings = []\n",
    "    for topping in toppings_list:\n",
    "        best_match, score = get_best_match(topping, model)\n",
    "        if best_match == 'toppings':\n",
    "            all_toppings.append({\n",
    "                \"NOT\": False if score > 0.5 else True,  # Assume NOT if score is low\n",
    "                \"Quantity\": None,  # Default quantity is None\n",
    "                \"Topping\": topping\n",
    "            })\n",
    "    return all_toppings\n",
    "\n",
    "def process_pizza_order(tokens, model):\n",
    "    \n",
    "    current_toppings = []\n",
    "    for token in tokens:\n",
    "        best_match, score = get_best_match(token, model)\n",
    "        if best_match == \"number\":\n",
    "            pizza_order_entry[\"NUMBER\"] = token\n",
    "        elif best_match == \"size\":\n",
    "            pizza_order_entry[\"SIZE\"] = token\n",
    "        elif best_match == \"style\":\n",
    "            pizza_order_entry[\"STYLE\"] = token\n",
    "        elif best_match == \"toppings\":\n",
    "            current_toppings.append(token)\n",
    "\n",
    "    pizza_order_entry[\"AllTopping\"] = process_toppings(current_toppings, model)\n",
    "    return pizza_order_entry\n",
    "\n",
    "def process_drink_order(tokens, model):\n",
    " \n",
    "    for token in tokens:\n",
    "        best_match, score = get_best_match(token, model)\n",
    "        if best_match == \"number\":\n",
    "            drink_order_entry[\"NUMBER\"] = token\n",
    "        elif best_match == \"size\":\n",
    "            drink_order_entry[\"SIZE\"] = token\n",
    "        elif best_match == \"drink_type\":\n",
    "            drink_order_entry[\"DRINKTYPE\"] = token\n",
    "        elif best_match == \"container_type\":\n",
    "            drink_order_entry[\"CONTAINERTYPE\"] = token\n",
    "\n",
    "    return drink_order_entry\n",
    "\n",
    "def parse_input(input_tokens, model):\n",
    "    pizza_order = {}  # To hold the single pizza order\n",
    "    drink_order = {}  # To hold the single drink order\n",
    "    \n",
    "    current_tokens = []\n",
    "    current_order = 0 # 0 for pizza, 1 for drink\n",
    "    for token in input_tokens:\n",
    "        best_match, score = get_best_match(token, model)\n",
    "        if best_match == \"container_type\" or best_match == \"drink_type\":\n",
    "            current_order = 1\n",
    "        if best_match == \"none\":\n",
    "            if current_order == 0:\n",
    "                pizza_order = process_pizza_order(current_tokens, model)\n",
    "            elif current_order == 1:\n",
    "                drink_order = process_drink_order(current_tokens, model)\n",
    "            # current_tokens = []\n",
    "        else:\n",
    "            current_tokens.append(token)\n",
    "    \n",
    "    # Process any remaining tokens\n",
    "    if current_tokens:\n",
    "        if current_order == 0:\n",
    "            pizza_order = process_pizza_order(current_tokens, model)\n",
    "        elif current_order == 1:\n",
    "            drink_order = process_drink_order(current_tokens, model)\n",
    "            \n",
    "    return pizza_order, drink_order\n",
    "\n",
    "\n",
    "input_text = \"i'd like a small pizza with pineapple buffalo chicken and garlic powder\"\n",
    "input_tokens = word_tokenize(input_text.lower())\n",
    "\n",
    "# Process the input tokens to generate orders\n",
    "pizza_order, drink_order = parse_input(input_tokens, model)  # Replace 'None' with your model\n",
    "\n",
    "# Final structured output\n",
    "final_order = {\n",
    "    \"ORDER\": {\n",
    "        \"PIZZAORDER\": [pizza_order],  # Wrap pizza order in a list\n",
    "        \"DRINKORDER\": [drink_order]  # Wrap drink order in a list\n",
    "    }\n",
    "}\n",
    "\n",
    "# Output the final structured order in JSON format\n",
    "import json\n",
    "print(json.dumps(final_order, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Label(Enum):\n",
    "    TOPPING = 0\n",
    "    NUMBER = 1\n",
    "    SIZE = 2\n",
    "    QUANTITY = 3\n",
    "    STYLE = 4\n",
    "    DRINKTYPE = 5\n",
    "    CONTAINERTYPE = 6\n",
    "    NONE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', <Label.NONE: 7>), (\"'d\", <Label.NONE: 7>), ('like', <Label.NONE: 7>), ('three', <Label.NUMBER: 1>), ('large', <Label.SIZE: 2>), ('pies', <Label.NONE: 7>), ('with', <Label.NONE: 7>), ('pestos', <Label.TOPPING: 0>), ('and', <Label.NONE: 7>), ('yellow', <Label.NONE: 7>), ('peppers', <Label.TOPPING: 0>)]\n",
      "[('i', <Label.NONE: 7>), (\"'d\", <Label.NONE: 7>), ('like', <Label.NONE: 7>), ('a', <Label.NUMBER: 1>), ('small', <Label.SIZE: 2>), ('pizza', <Label.NONE: 7>), ('with', <Label.NONE: 7>), ('pineapple', <Label.TOPPING: 0>), ('buffalo', <Label.NONE: 7>), ('chicken', <Label.TOPPING: 0>), ('and', <Label.NONE: 7>), ('garlic', <Label.TOPPING: 0>), ('powder', <Label.NONE: 7>)]\n",
      "[('three', <Label.NUMBER: 1>), ('party', <Label.NONE: 7>), ('sized', <Label.NONE: 7>), ('pizzas', <Label.NONE: 7>), ('with', <Label.NONE: 7>), ('pickles', <Label.TOPPING: 0>), ('and', <Label.NONE: 7>), ('hot', <Label.NONE: 7>), ('pepper', <Label.TOPPING: 0>)]\n",
      "[('balsamic', <Label.NONE: 7>), ('glaze', <Label.NONE: 7>), ('and', <Label.NONE: 7>), ('a', <Label.NUMBER: 1>), ('sprite', <Label.DRINKTYPE: 5>), ('and', <Label.NONE: 7>), ('three', <Label.NUMBER: 1>), ('eight', <Label.NUMBER: 1>), ('ounce', <Label.NONE: 7>), ('fantas', <Label.DRINKTYPE: 5>), ('and', <Label.NONE: 7>), ('three', <Label.NUMBER: 1>), ('sprites', <Label.DRINKTYPE: 5>)]\n",
      "[('i', <Label.NONE: 7>), (\"'d\", <Label.NONE: 7>), ('like', <Label.NONE: 7>), ('a', <Label.NUMBER: 1>), ('pizza', <Label.NONE: 7>), ('with', <Label.NONE: 7>), ('artichoke', <Label.TOPPING: 0>), ('meatballs', <Label.TOPPING: 0>), ('and', <Label.NONE: 7>), ('ricotta', <Label.TOPPING: 0>), ('without', <Label.NONE: 7>), ('thin', <Label.NONE: 7>), ('crust', <Label.NONE: 7>)]\n"
     ]
    }
   ],
   "source": [
    "def label_word(word):\n",
    "    if word in toppings:\n",
    "        return Label.TOPPING\n",
    "    elif word in numbers:\n",
    "        return Label.NUMBER\n",
    "    elif word in sizes:\n",
    "        return Label.SIZE\n",
    "    elif word in quantities:\n",
    "        return Label.QUANTITY\n",
    "    elif word in styles:\n",
    "        return Label.STYLE\n",
    "    elif word in drink_types:\n",
    "        return Label.DRINKTYPE\n",
    "    elif word in container_types:\n",
    "        return Label.CONTAINERTYPE\n",
    "    else:\n",
    "        return Label.NONE\n",
    "\n",
    "# Example usage\n",
    "labeled_data = []\n",
    "for sentence in sentences:\n",
    "    labeled_sentence = [(word, label_word(word)) for word in sentence]\n",
    "    labeled_data.append(labeled_sentence)\n",
    "\n",
    "# Print the labeled data\n",
    "for sentence in labeled_data[:5]:  # Print first 5 sentences for brevity\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Extract sentences and labels\n",
    "sentences = [\" \".join(word for word, label in sentence) for sentence in labeled_data]\n",
    "labels = [[label.value for _, label in sentence] for sentence in labeled_data]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the tokenized sentences and labels\n",
    "max_len = max(len(seq) for seq in tokenized_sentences)\n",
    "X = pad_sequences(tokenized_sentences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "y = pad_sequences(labels, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Convert y to categorical format\n",
    "num_classes = len(Label)\n",
    "y = np.eye(num_classes)[y]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m38,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │         \u001b[38;5;34m1,032\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,248</span> (540.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,248\u001b[0m (540.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,248</span> (540.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m138,248\u001b[0m (540.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define model parameters\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(embedding_layer)\n",
    "output_layer = TimeDistributed(Dense(num_classes, activation=\"softmax\"))(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8569 - loss: 0.5031\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9826 - loss: 0.0464\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9836 - loss: 0.0367\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9835 - loss: 0.0335\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9838 - loss: 0.0318\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9839 - loss: 0.0308\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9842 - loss: 0.0299\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9844 - loss: 0.0292\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9846 - loss: 0.0287\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9849 - loss: 0.0281\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Test Sequences: [[1, 10, 6, 30, 13, 3, 118, 2, 83, 31]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Raw Predictions: [[[9.65911001e-02 1.13209322e-01 1.16967224e-02 1.50052970e-02\n",
      "   9.83704776e-02 1.88730452e-02 2.10612901e-02 6.25192702e-01]\n",
      "  [7.44007775e-05 2.47842778e-04 8.11098107e-06 9.96168546e-06\n",
      "   1.19190972e-05 2.58426007e-06 5.31987962e-06 9.99639869e-01]\n",
      "  [1.23406771e-05 9.99737561e-01 8.66172286e-07 3.32436048e-06\n",
      "   7.02610009e-07 3.95278789e-07 6.46776755e-07 2.44217168e-04]\n",
      "  [9.87180442e-07 5.83189376e-06 9.99963760e-01 3.48238927e-06\n",
      "   1.11665613e-05 1.50087408e-06 3.46256115e-07 1.29065902e-05]\n",
      "  [8.01549031e-05 1.05756726e-06 8.24330891e-06 1.58563410e-06\n",
      "   6.23440428e-06 2.63829378e-07 3.81852573e-07 9.99902129e-01]\n",
      "  [4.51901578e-05 1.91986071e-08 1.08349205e-08 5.64815750e-08\n",
      "   1.83516811e-08 6.15472739e-09 1.15833600e-08 9.99954820e-01]\n",
      "  [9.99727428e-01 9.58483156e-07 7.53720197e-09 2.66237726e-06\n",
      "   8.07356230e-07 3.00701174e-07 4.89987201e-07 2.67324358e-04]\n",
      "  [1.88463222e-04 9.81193239e-07 4.28980718e-09 9.53198267e-08\n",
      "   1.39396237e-08 1.77636714e-08 1.95760862e-07 9.99810278e-01]\n",
      "  [1.23130204e-03 9.95778350e-07 2.62532694e-07 4.64355981e-06\n",
      "   2.91467791e-06 5.23222184e-07 3.07560208e-06 9.98756289e-01]\n",
      "  [9.99627113e-01 5.26531630e-06 1.73409251e-08 1.52691682e-05\n",
      "   6.15439285e-06 5.45398234e-06 1.88407125e-06 3.38753365e-04]\n",
      "  [9.99988556e-01 3.11073052e-07 2.54540694e-10 3.44545214e-07\n",
      "   5.20893082e-07 7.50599156e-07 6.05109562e-08 9.49613514e-06]\n",
      "  [9.99995947e-01 1.67768050e-07 1.41126263e-10 1.40387769e-07\n",
      "   2.56844885e-07 3.49541324e-07 2.53968651e-08 3.13882310e-06]\n",
      "  [9.99996424e-01 1.81554995e-07 2.12792006e-10 1.23794990e-07\n",
      "   2.55987658e-07 3.46799823e-07 2.86428996e-08 2.63775701e-06]\n",
      "  [9.99995947e-01 2.15371415e-07 3.08588738e-10 1.38820752e-07\n",
      "   3.04611490e-07 3.99807021e-07 3.78010405e-08 2.88664660e-06]\n",
      "  [9.99995947e-01 2.24247259e-07 3.37704642e-10 1.46515191e-07\n",
      "   3.25821418e-07 4.15382402e-07 4.13105532e-08 2.93474272e-06]\n",
      "  [9.99995947e-01 2.22779022e-07 3.38743283e-10 1.49848304e-07\n",
      "   3.34954706e-07 4.14085861e-07 4.26130278e-08 2.90740059e-06]\n",
      "  [9.99995947e-01 2.19815348e-07 3.36535660e-10 1.52624082e-07\n",
      "   3.41558831e-07 4.11135829e-07 4.35876402e-08 2.86890099e-06]\n",
      "  [9.99996066e-01 2.16455348e-07 3.36228989e-10 1.55227909e-07\n",
      "   3.45278380e-07 4.09481061e-07 4.43226362e-08 2.80608037e-06]\n",
      "  [9.99996185e-01 2.12118579e-07 3.40770384e-10 1.57387120e-07\n",
      "   3.42240384e-07 4.10715444e-07 4.45111397e-08 2.68251347e-06]\n",
      "  [9.99996424e-01 2.05832350e-07 3.56117691e-10 1.58822431e-07\n",
      "   3.23235724e-07 4.16892249e-07 4.33685656e-08 2.42472584e-06]\n",
      "  [9.99997139e-01 1.98952506e-07 3.91993549e-10 1.59489332e-07\n",
      "   2.69662451e-07 4.16501109e-07 3.85418666e-08 1.89834861e-06]\n",
      "  [9.99997854e-01 2.22393524e-07 4.83001028e-10 1.77975423e-07\n",
      "   1.89337328e-07 3.58906021e-07 2.86204411e-08 1.16185652e-06]\n",
      "  [9.99997497e-01 6.22115749e-07 1.30451538e-09 4.32829353e-07\n",
      "   2.52260151e-07 3.55757464e-07 3.08822656e-08 8.83573307e-07]\n",
      "  [9.99973297e-01 1.16167694e-05 2.36897879e-08 6.44269085e-06\n",
      "   3.84246096e-06 1.59708554e-06 2.52162152e-07 2.98727923e-06]]]\n",
      "Sentence: i'd like three large pies with pestos and yellow peppers\n",
      "i'd: NONE\n",
      "like: NONE\n",
      "three: NUMBER\n",
      "large: SIZE\n",
      "pies: NONE\n",
      "with: NONE\n",
      "pestos: TOPPING\n",
      "and: NONE\n",
      "yellow: NONE\n",
      "peppers: TOPPING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Assuming `test_sentences` is a list of test sentences\n",
    "test_sentences = [\"i'd like three large pies with pestos and yellow peppers\"]\n",
    "\n",
    "# Tokenize and convert to sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Print tokenized output for debugging\n",
    "print(\"Tokenized Test Sequences:\", test_sequences)\n",
    "\n",
    "# Use the same max sequence length as in training\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Predict with the trained model\n",
    "predictions = model.predict(test_sequences)\n",
    "\n",
    "# Print raw predictions\n",
    "print(\"Raw Predictions:\", predictions)\n",
    "\n",
    "\n",
    "# Get the index of the max probability for each timestep\n",
    "decoded_predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Map predictions back to label names using the Label Enum\n",
    "decoded_labels = [\n",
    "    [Label(pred).name for pred in sentence] for sentence in decoded_predictions\n",
    "]\n",
    "\n",
    "# Print the decoded labels\n",
    "for i in range(len(test_sentences)):\n",
    "    print(f\"Sentence: {test_sentences[i]}\")\n",
    "    for word, label in zip(test_sentences[i].split(), decoded_labels[i]):\n",
    "        print(f\"{word}: {label}\")\n",
    "    print()\n",
    "# print(\"Decoded Predictions:\", test_sentences)\n",
    "# print(\"Decoded Predictions:\", decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_order(decoded_labels, words):\n",
    "    order = {\"ORDER\": []}  # Root structure\n",
    "\n",
    "    current_drink_order = None  # Tracks the current drink order being built\n",
    "\n",
    "    for word, label in zip(words, decoded_labels):\n",
    "        if label == \"DRINKTYPE\":  # Start a new drink order\n",
    "            # Save the previous drink order if it exists\n",
    "            if current_drink_order:\n",
    "                order[\"ORDER\"].append(current_drink_order)\n",
    "            # Start a new one\n",
    "            current_drink_order = {\"DRINKORDER\": {\"DRINKTYPE\": word}}\n",
    "\n",
    "        elif label == \"NUMBER\":  # Add quantity\n",
    "            if current_drink_order:\n",
    "                current_drink_order[\"DRINKORDER\"][\"NUMBER\"] = word\n",
    "\n",
    "        elif label == \"SIZE\":  # Add size\n",
    "            if current_drink_order:\n",
    "                current_drink_order[\"DRINKORDER\"][\"SIZE\"] = word\n",
    "\n",
    "    # Add the last drink order if it exists\n",
    "    if current_drink_order:\n",
    "        order[\"ORDER\"].append(current_drink_order)\n",
    "\n",
    "    return order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_order(order):\n",
    "    def format_drink_order(drink_order):\n",
    "        formatted = \"(DRINKORDER \"\n",
    "        for key, value in drink_order[\"DRINKORDER\"].items():\n",
    "            formatted += f\"({key} {value.upper()} ) \"\n",
    "        return formatted.strip() + \")\"\n",
    "    \n",
    "\n",
    "    formatted_order = \"(ORDER \"\n",
    "    for drink_order in order[\"ORDER\"]:\n",
    "        formatted_order += format_drink_order(drink_order) + \" \"\n",
    "    return formatted_order.strip() + \")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORDER)\n"
     ]
    }
   ],
   "source": [
    "words = test_sentences[0].split()\n",
    "\n",
    "# Build the order structure\n",
    "order_structure = build_order(decoded_labels, words)\n",
    "\n",
    "# Format the structure as a string\n",
    "formatted_output = format_order(order_structure)\n",
    "\n",
    "# Print the formatted output\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
