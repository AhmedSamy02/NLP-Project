{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Dot, Activation, Lambda\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_train = pd.read_json(\"dataset/PIZZA_train.json\", lines=True,)\n",
    "df_dev = pd.read_json(\"dataset/PIZZA_dev.json\", lines=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = main_train.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \n",
    "    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
    "    \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13,\n",
    "    \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17,\n",
    "    \"eighteen\": 18, \"nineteen\": 19, \"twenty\": 20,\n",
    "    \"thirty\": 30, \"forty\": 40, \"fifty\": 50, \"sixty\": 60,\n",
    "    \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n",
    "    \"hundred\": 100\n",
    "}\n",
    "\n",
    "\n",
    "def words_to_number(word):\n",
    "    word = word.lower().strip()\n",
    "    \n",
    "    # Handle simple numbers directly\n",
    "    if word in word_to_num:\n",
    "        return word_to_num[word]\n",
    "    \n",
    "    # Handle composite numbers (e.g., twenty-one)\n",
    "    if \"-\" in word:\n",
    "        parts = word.split(\"-\")\n",
    "        return sum(word_to_num[part] for part in parts)\n",
    "    \n",
    "    # Handle \"hundred\" cases (e.g., one hundred twenty-three)\n",
    "    if \"hundred\" in word:\n",
    "        parts = word.split(\"hundred\")\n",
    "        hundreds = word_to_num[parts[0].strip()] * 100\n",
    "        if parts[1].strip():  # If there's something after \"hundred\"\n",
    "            return hundreds + words_to_number(parts[1].strip())\n",
    "        return hundreds\n",
    "    \n",
    "    return None  # Return None if the input is not a valid number word\n",
    "\n",
    "def standardize_numbers(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # Replace number words with digits\n",
    "    standardized_tokens = [\n",
    "        str(words_to_number(token)) if words_to_number(token) is not None else token\n",
    "        for token in tokens\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(standardized_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w']\", \" \", text)  # Remove non-word characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = text.lower().strip()  # Lowercase and strip whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma(text):\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "    \"n't\": \"not\",\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'d\": \"would\",\n",
    "    \"'em\": \"them\",\n",
    "    \"'all\": \"all\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"'clock\": \"oclock\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"'tween\": \"between\",\n",
    "    \"'twere\": \"it were\",\n",
    "    \"'twould\": \"it would\",\n",
    "    \"'twixt\": \"betwixt\",\n",
    "    \"'twill\": \"it will\",\n",
    "    \"'til\": \"until\",\n",
    "    \"'bout\": \"about\",\n",
    "    \"'cept\": \"except\",\n",
    "    \"'cos\": \"because\",\n",
    "    \"'fore\": \"before\",\n",
    "    \"'round\": \"around\",\n",
    "    \"'n'\": \"and\",\n",
    "    \"'neath\": \"beneath\",\n",
    "    \"'nother\": \"another\",\n",
    "    \"'nuff\": \"enough\",\n",
    "}\n",
    "def expnad_abb2(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"(\" + \"|\".join(re.escape(key) for key in CONTRACTIONS.keys()) + r\")\"\n",
    "    )\n",
    "    expanded_text = pattern.sub(lambda x: \" \" + CONTRACTIONS[x.group()], text)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = {\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"none\",\n",
    "    \"never\",\n",
    "    \"without\",\n",
    "    \"avoid\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"hate\",\n",
    "    \"hold\",\n",
    "}\n",
    "pizza = {\"pizza\", \"pizzas\", \"pie\", \"pies\"}\n",
    "\n",
    "stop_negation_words = {\"and\", \"but\"}\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words - negation_words - stop_negation_words - {'all'}\n",
    "stop_words.update({\"would\", \"like\", \"get\", \"want\"})\n",
    "stop_words.update(pizza)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_negation(text):\n",
    "    text = text.split()\n",
    "    for i, word in enumerate(text):\n",
    "        if word in negation_words:\n",
    "            text[i] = 'not'\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "input_texts = df_train[\"train.SRC\"].apply(clean_text).apply(lemma).apply(standardize_numbers).apply(expnad_abb2).apply(remove_stopwords).apply(std_negation)\n",
    "output_texts = ['<SOS> ' + token + ' <EOS>' for token in df_train[\"train.EXR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Multiply, Add, Bidirectional\n",
    "\n",
    "def pointer_generator_model(input_vocab_size, output_vocab_size, max_input_length=100,max_output_length=100 ,embedding_dim=128, hidden_units=256):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(max_input_length,), name='encoder_input')\n",
    "    encoder_embedding = Embedding(input_vocab_size, embedding_dim, name='encoder_embedding')(encoder_input)\n",
    "    encoder_lstm = Bidirectional(LSTM(hidden_units, return_sequences=True, return_state=True, name='encoder_lstm'))\n",
    "    encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_state_h = Concatenate()([forward_h, backward_h])\n",
    "    encoder_state_c = Concatenate()([forward_c, backward_c])\n",
    "    # encoder_output, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(max_output_length,), name='decoder_input')\n",
    "    decoder_embedding = Embedding(output_vocab_size, embedding_dim, name='decoder_embedding')(decoder_input)\n",
    "    decoder_lstm = LSTM(hidden_units*2, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    # Attention Mechanism\n",
    "    attention_dot = Dot(axes=[2, 2], name='attention_dot')([decoder_output, encoder_output])\n",
    "    attention_activation = Activation('softmax', name='attention_activation')(attention_dot)\n",
    "    context_vector = Dot(axes=[2, 1], name='context_vector')([attention_activation, encoder_output])\n",
    "\n",
    "    # Pointer mechanism (Generate + Copy)\n",
    "    context_decoder_combined = Concatenate(axis=-1, name='context_decoder_combined')([context_vector, decoder_output])\n",
    "    pointer_vocab_distribution = Dense(output_vocab_size, activation='softmax', name='pointer_vocab_distribution')(context_decoder_combined)\n",
    "    p_gen = Dense(1, activation='sigmoid', name='p_gen')(context_decoder_combined)\n",
    "\n",
    "    # Final output distribution\n",
    "    final_vocab_distribution = Multiply()([pointer_vocab_distribution, p_gen])  # Shape: [batch_size, time_steps, vocab_size]\n",
    "\n",
    "    # Project attention distribution to match vocab size\n",
    "    # Project attention distribution to match vocab size\n",
    "    attention_projection = Dense(output_vocab_size, activation='softmax')(attention_activation)\n",
    "    final_attention_distribution = Multiply()([attention_projection, 1 - p_gen])  # Shape: [batch_size, time_steps, vocab_size]\n",
    "\n",
    "    # Combine the two distributions\n",
    "    final_output = Add(name='final_output')([final_vocab_distribution, final_attention_distribution])  # Shape: [batch_size, time_steps, vocab_size]\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=final_output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 219\n",
      "Output vocab size: 184\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizers\n",
    "input_tokenizer = Tokenizer(filters='', lower=False, oov_token = \"<OOV>\")  # We don't want to lowercase for structured text\n",
    "output_tokenizer = Tokenizer(filters='', lower = False , oov_token = \"<OOV>\")  # Same for output\n",
    "\n",
    "# Fit the tokenizers on the text data\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "output_tokenizer.fit_on_texts(output_texts)\n",
    "\n",
    "input_tokenizer.word_index['<PAD>'] = 0\n",
    "input_tokenizer.index_word[0] = '<PAD>'\n",
    "output_tokenizer.word_index['<PAD>'] = 0\n",
    "output_tokenizer.index_word[0] = '<PAD>'\n",
    "\n",
    "# input_tokenizer.word_index['<OOV>'] = len(input_tokenizer.word_index) + 1\n",
    "# input_tokenizer.index_word[len(input_tokenizer.word_index) + 1] = '<OOV>'\n",
    "# output_tokenizer.word_index['<OOV>'] = len(output_tokenizer.word_index) + 1\n",
    "# output_tokenizer.index_word[len(output_tokenizer.word_index) + 1] = '<OOV>'\n",
    "\n",
    "\n",
    "# Convert texts to sequences (convert to integer sequences)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
    "\n",
    "# Padding sequences to ensure uniform length (adjust max_input_length and max_output_length as needed)\n",
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "max_output_length = max(len(seq) for seq in output_sequences)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=100, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=100, padding='post')\n",
    "\n",
    "# Convert output sequences into one-hot format for training\n",
    "# output_sequences_one_hot = np.array([np.expand_dims(seq, axis=-1) for seq in output_sequences])  # For sparse categorical loss\n",
    "\n",
    "# Get vocab sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1  # Include padding token\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1  # Include padding token\n",
    "\n",
    "print(f'Input vocab size: {input_vocab_size}')\n",
    "print(f'Output vocab size: {output_vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input sequence shape: (100000, 100)\n",
      "Decoder output sequence shape: (100000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare decoder input and output sequences for training\n",
    "decoder_input_sequences = np.zeros_like(output_sequences)  # Start tokens will be placed here\n",
    "decoder_input_sequences[:, 1:] = output_sequences[:, :-1]  # Shifted sequence (remove first token)\n",
    "\n",
    "# The target is the original output sequence\n",
    "decoder_output_sequences = output_sequences  # For teacher forcing\n",
    "\n",
    "print(f'Decoder input sequence shape: {decoder_input_sequences.shape}')\n",
    "print(f'Decoder output sequence shape: {decoder_output_sequences.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input data shape: (100000, 100)\n",
      "Decoder input data shape: (100000, 100)\n",
      "Decoder output data shape: (100000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model inputs\n",
    "encoder_input_data = input_sequences  # Encoder input (tokenized input sentences)\n",
    "decoder_input_data = decoder_input_sequences  # Decoder input (shifted target sequences)\n",
    "\n",
    "# Prepare the target data for the decoder (one-hot encoded)\n",
    "decoder_output_data = decoder_output_sequences  # Use integer sequences\n",
    "\n",
    "print(f'Encoder input data shape: {encoder_input_data.shape}')\n",
    "print(f'Decoder input data shape: {decoder_input_data.shape}')\n",
    "print(f'Decoder output data shape: {decoder_output_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " encoder_embedding (Embeddi  (None, 100, 128)             28032     ['encoder_input[0][0]']       \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)  [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  [(None, 100, 512),           788480    ['encoder_embedding[0][0]']   \n",
      " onal)                        (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " decoder_embedding (Embeddi  (None, 100, 128)             23552     ['decoder_input[0][0]']       \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 512)                  0         ['bidirectional_5[0][1]',     \n",
      " e)                                                                  'bidirectional_5[0][3]']     \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenat  (None, 512)                  0         ['bidirectional_5[0][2]',     \n",
      " e)                                                                  'bidirectional_5[0][4]']     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         [(None, 100, 512),           1312768   ['decoder_embedding[0][0]',   \n",
      "                              (None, 512),                           'concatenate_10[0][0]',      \n",
      "                              (None, 512)]                           'concatenate_11[0][0]']      \n",
      "                                                                                                  \n",
      " attention_dot (Dot)         (None, 100, 100)             0         ['decoder_lstm[0][0]',        \n",
      "                                                                     'bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " attention_activation (Acti  (None, 100, 100)             0         ['attention_dot[0][0]']       \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " context_vector (Dot)        (None, 100, 512)             0         ['attention_activation[0][0]',\n",
      "                                                                     'bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " context_decoder_combined (  (None, 100, 1024)            0         ['context_vector[0][0]',      \n",
      " Concatenate)                                                        'decoder_lstm[0][0]']        \n",
      "                                                                                                  \n",
      " p_gen (Dense)               (None, 100, 1)               1025      ['context_decoder_combined[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " pointer_vocab_distribution  (None, 100, 184)             188600    ['context_decoder_combined[0][\n",
      "  (Dense)                                                           0]']                          \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 100, 184)             18584     ['attention_activation[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.subtract_11 (TFOpL  (None, 100, 1)               0         ['p_gen[0][0]']               \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " multiply_23 (Multiply)      (None, 100, 184)             0         ['pointer_vocab_distribution[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'p_gen[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_24 (Multiply)      (None, 100, 184)             0         ['dense_10[0][0]',            \n",
      "                                                                     'tf.math.subtract_11[0][0]'] \n",
      "                                                                                                  \n",
      " final_output (Add)          (None, 100, 184)             0         ['multiply_23[0][0]',         \n",
      "                                                                     'multiply_24[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2361041 (9.01 MB)\n",
      "Trainable params: 2361041 (9.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and summarize the model\n",
    "model = pointer_generator_model(input_vocab_size, output_vocab_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(decoder_output_sequences \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Zero weight for padding\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_output_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_weights = np.where(decoder_output_sequences == 0, 0.0, 1.0)  # Zero weight for padding\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_output_data,\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:  3 large pesto and yellow pepper\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted Output:  <SOS> (ORDER (PIZZAORDER (NUMBER (NUMBER ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def predict_sequence(input_text, model, input_tokenizer, output_tokenizer,max_input_length=100,max_output_length= 100):\n",
    "\n",
    "    # Tokenize and pad the input text (same as during training)\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=max_input_length, padding='post')\n",
    "\n",
    "    # Prepare initial decoder input (start with <SOS> token)\n",
    "    decoder_input = np.zeros((1, max_output_length))\n",
    "    decoder_input[0, 0] = output_tokenizer.word_index['<SOS>']\n",
    "\n",
    "    # Prepare to collect the prediction\n",
    "    predicted_sequence = []\n",
    "\n",
    "    # Decoder loop for predicting one token at a time\n",
    "    for t in range(1, max_output_length):\n",
    "        # Predict the next token in the sequence\n",
    "        predictions = model.predict([input_sequence, decoder_input])\n",
    "\n",
    "        # Extract the predicted token and add it to the predicted sequence\n",
    "        predicted_token_idx = np.argmax(predictions[0, t-1, :])  # Get the token with highest probability\n",
    "        predicted_sequence.append(predicted_token_idx)\n",
    "\n",
    "        # Stop if <EOS> token is predicted\n",
    "        if predicted_token_idx == output_tokenizer.word_index['<EOS>']:\n",
    "            break\n",
    "\n",
    "        # Update the decoder input with the predicted token (for the next timestep)\n",
    "        decoder_input[0, t] = predicted_token_idx\n",
    "\n",
    "    # Convert token indices to words using the tokenizer\n",
    "    predicted_text = ' '.join([output_tokenizer.index_word.get(idx, '<OOV>') for idx in predicted_sequence])\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_text = \"i'd like three large pies with pestos and yellow peppers\"\n",
    "input_text = std_negation(remove_stopwords(expnad_abb2(standardize_numbers(lemma(clean_text(input_text))))))\n",
    "print(\"Input Text: \", input_text)   \n",
    "predicted_output = predict_sequence(input_text, model, input_tokenizer, output_tokenizer)\n",
    "\n",
    "print(\"Predicted Output: \", predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "<SOS> (order (pizzaorder (pizzaorder (number (pizzaorder (pizzaorder (pizzaorder (pizzaorder (pizzaorder <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Example function for inference\n",
    "def predict_order(input_sentence, model, tokenizer, max_input_length, max_target_length):\n",
    "    # Tokenize the input sentence (encoder input)\n",
    "    encoder_input_seq = tokenizer.texts_to_sequences([input_sentence])\n",
    "    encoder_input_seq = pad_sequences(encoder_input_seq, maxlen=max_input_length, padding='post')\n",
    "    \n",
    "    # Initialize the decoder input with the <START> token\n",
    "    decoder_input_seq = np.zeros((1, max_target_length))\n",
    "    decoder_input_seq[0, 0] = output_tokenizer.word_index['<SOS>']  # Assuming <START> token has index 1\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict([encoder_input_seq, decoder_input_seq])\n",
    "    \n",
    "    # Convert predicted tokens back to words (or structured output)\n",
    "    predicted_sequence = decode_output(prediction, output_tokenizer)\n",
    "    return predicted_sequence\n",
    "\n",
    "# Example of how to decode the output into a structured form\n",
    "def decode_output(prediction, tokenizer):\n",
    "    # Convert the predicted probabilities into the most likely token indices\n",
    "    predicted_tokens = np.argmax(prediction, axis=-1)\n",
    "    \n",
    "    # Map token indices back to words using the tokenizer's word_index (inverse mapping)\n",
    "    reverse_word_index = {i: word for word, i in tokenizer.word_index.items()}\n",
    "    predicted_words = [reverse_word_index.get(i, '<OOV>') for i in predicted_tokens[0]]\n",
    "    \n",
    "    # Join the words into the structured output\n",
    "    structured_output = ' '.join(predicted_words)\n",
    "    return structured_output\n",
    "\n",
    "# Example input sentence\n",
    "input_sentence = \"i'd like three large pies with pestos and yellow peppers\"\n",
    "predicted_output = predict_order(input_sentence, model, input_tokenizer, max_input_length=max_input_length, max_target_length=max_output_length)\n",
    "print(predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'with': 2,\n",
       " 'a': 3,\n",
       " 'three': 4,\n",
       " 'pizzas': 5,\n",
       " 'pizza': 6,\n",
       " \"i'd\": 7,\n",
       " 'like': 8,\n",
       " 'cheese': 9,\n",
       " 'four': 10,\n",
       " 'pies': 11,\n",
       " 'party': 12,\n",
       " 'five': 13,\n",
       " 'american': 14,\n",
       " 'sized': 15,\n",
       " 'one': 16,\n",
       " 'no': 17,\n",
       " 'of': 18,\n",
       " 'two': 19,\n",
       " 'i': 20,\n",
       " 'size': 21,\n",
       " 'sprite': 22,\n",
       " 'pepper': 23,\n",
       " 'glaze': 24,\n",
       " 'ice': 25,\n",
       " 'without': 26,\n",
       " 'ounce': 27,\n",
       " 'balsamic': 28,\n",
       " '-': 29,\n",
       " 'large': 30,\n",
       " 'peppers': 31,\n",
       " 'pie': 32,\n",
       " 'crust': 33,\n",
       " 'tea': 34,\n",
       " 'thin': 35,\n",
       " 'sauce': 36,\n",
       " 'green': 37,\n",
       " 'ups': 38,\n",
       " 'personal': 39,\n",
       " 'extra': 40,\n",
       " 'diet': 41,\n",
       " 'medium': 42,\n",
       " 'roasted': 43,\n",
       " 'seven': 44,\n",
       " 'teas': 45,\n",
       " 'red': 46,\n",
       " 'also': 47,\n",
       " 'pecorino': 48,\n",
       " 'peperonni': 49,\n",
       " 'cans': 50,\n",
       " 'ginger': 51,\n",
       " 'banana': 52,\n",
       " 'need': 53,\n",
       " 'any': 54,\n",
       " 'chicken': 55,\n",
       " 'fantas': 56,\n",
       " 'little': 57,\n",
       " 'lunch': 58,\n",
       " 'sprites': 59,\n",
       " '500': 60,\n",
       " 'bottle': 61,\n",
       " 'mozzarella': 62,\n",
       " 'ale': 63,\n",
       " 'onions': 64,\n",
       " 'can': 65,\n",
       " '20': 66,\n",
       " 'the': 67,\n",
       " 'coke': 68,\n",
       " 'want': 69,\n",
       " 'onion': 70,\n",
       " 'bit': 71,\n",
       " 'milliliter': 72,\n",
       " 'olive': 73,\n",
       " 'hold': 74,\n",
       " 'have': 75,\n",
       " 'just': 76,\n",
       " 'liter': 77,\n",
       " 'pineapple': 78,\n",
       " 'regular': 79,\n",
       " 'pellegrino': 80,\n",
       " 'fl': 81,\n",
       " 'yellow': 82,\n",
       " 'iced': 83,\n",
       " 'san': 84,\n",
       " 'olives': 85,\n",
       " 'not': 86,\n",
       " 'eight': 87,\n",
       " 'ml': 88,\n",
       " 'tomato': 89,\n",
       " 'dried': 90,\n",
       " 'pellegrinos': 91,\n",
       " 'caramelized': 92,\n",
       " 'buffalo': 93,\n",
       " 'fluid': 94,\n",
       " 'lot': 95,\n",
       " 'bacon': 96,\n",
       " 'dr': 97,\n",
       " 'pepperoni': 98,\n",
       " 'in': 99,\n",
       " 'tomatoes': 100,\n",
       " 'garlic': 101,\n",
       " '12': 102,\n",
       " 'lemon': 103,\n",
       " 'mountain': 104,\n",
       " 'barbecue': 105,\n",
       " 'meatball': 106,\n",
       " 'small': 107,\n",
       " '200': 108,\n",
       " '8': 109,\n",
       " 'bbq': 110,\n",
       " 'pesto': 111,\n",
       " 'fried': 112,\n",
       " 'sausage': 113,\n",
       " 'waters': 114,\n",
       " 'cherry': 115,\n",
       " 'much': 116,\n",
       " '16': 117,\n",
       " 'pestos': 118,\n",
       " 'vegan': 119,\n",
       " 'pepsis': 120,\n",
       " '7': 121,\n",
       " 'doctor': 122,\n",
       " 'beef': 123,\n",
       " 'kalamata': 124,\n",
       " 'feta': 125,\n",
       " 'jalapeno': 126,\n",
       " 'fanta': 127,\n",
       " 'mozarella': 128,\n",
       " 'zeros': 129,\n",
       " 'hot': 130,\n",
       " 'peppperoni': 131,\n",
       " 'only': 132,\n",
       " 'mushrooms': 133,\n",
       " 'many': 134,\n",
       " 'lots': 135,\n",
       " 'low': 136,\n",
       " 'fat': 137,\n",
       " 'pulled': 138,\n",
       " 'pork': 139,\n",
       " 'grilled': 140,\n",
       " 'sodas': 141,\n",
       " 'tiny': 142,\n",
       " 'peper': 143,\n",
       " 'perriers': 144,\n",
       " 'black': 145,\n",
       " 'parmesan': 146,\n",
       " 'arugula': 147,\n",
       " 'pepers': 148,\n",
       " 'pepsi': 149,\n",
       " 'crusts': 150,\n",
       " 'soda': 151,\n",
       " 'peperronni': 152,\n",
       " 'dew': 153,\n",
       " 'high': 154,\n",
       " 'rise': 155,\n",
       " 'dough': 156,\n",
       " 'stuffed': 157,\n",
       " 'dews': 158,\n",
       " 'zeroes': 159,\n",
       " 'pickles': 160,\n",
       " 'ricotta': 161,\n",
       " 'balzamic': 162,\n",
       " 'hams': 163,\n",
       " 'white': 164,\n",
       " '500-ml': 165,\n",
       " 'ranch': 166,\n",
       " 'ales': 167,\n",
       " 'artichokes': 168,\n",
       " 'ham': 169,\n",
       " 'perrier': 170,\n",
       " 'bottles': 171,\n",
       " 'cheeseburger': 172,\n",
       " 'apple': 173,\n",
       " 'wood': 174,\n",
       " 'artichoke': 175,\n",
       " 'carrots': 176,\n",
       " 'bacons': 177,\n",
       " 'alfredo': 178,\n",
       " '500-milliliter': 179,\n",
       " 'chorizo': 180,\n",
       " 'meat': 181,\n",
       " 'oz': 182,\n",
       " 'powder': 183,\n",
       " 'applewood': 184,\n",
       " 'oregano': 185,\n",
       " 'peperroni': 186,\n",
       " 'peperoni': 187,\n",
       " 'chorrizo': 188,\n",
       " 'anchovies': 189,\n",
       " 'avoid': 190,\n",
       " 'hate': 191,\n",
       " 'three-liter': 192,\n",
       " 'bay': 193,\n",
       " 'leaves': 194,\n",
       " 'bean': 195,\n",
       " 'brocoli': 196,\n",
       " 'shrimps': 197,\n",
       " 'broccoli': 198,\n",
       " 'beans': 199,\n",
       " 'ground': 200,\n",
       " 'two-liter': 201,\n",
       " 'sixteen': 202,\n",
       " 'anchovy': 203,\n",
       " 'cumin': 204,\n",
       " 'spiced': 205,\n",
       " 'chickens': 206,\n",
       " 'mushroom': 207,\n",
       " '200-milliliter': 208,\n",
       " 'basil': 209,\n",
       " 'lettuce': 210,\n",
       " 'peppperonis': 211,\n",
       " 'carrot': 212,\n",
       " 'spinach': 213,\n",
       " 'tuna': 214,\n",
       " 'italian': 215,\n",
       " 'pickle': 216,\n",
       " 'cheddar': 217,\n",
       " 'oil': 218,\n",
       " 'big': 219,\n",
       " 'rosemary': 220,\n",
       " 'pineaple': 221,\n",
       " 'parsley': 222,\n",
       " 'pepperonis': 223,\n",
       " 'pineapples': 224,\n",
       " 'flakes': 225,\n",
       " 'spicy': 226,\n",
       " 'peas': 227,\n",
       " 'shrimp': 228,\n",
       " 'flake': 229,\n",
       " 'thick': 230,\n",
       " 'meatlover': 231,\n",
       " 'pea': 232,\n",
       " 'all': 233,\n",
       " 'peperonis': 234,\n",
       " '3': 235,\n",
       " 'salami': 236,\n",
       " 'tunas': 237,\n",
       " 'jalapenos': 238,\n",
       " 'water': 239,\n",
       " 'pineaples': 240,\n",
       " 'eleven': 241,\n",
       " 'up': 242,\n",
       " 'ten': 243,\n",
       " '10': 244,\n",
       " '2': 245,\n",
       " '9': 246,\n",
       " '5': 247,\n",
       " 'sausages': 248,\n",
       " 'six': 249,\n",
       " '11': 250,\n",
       " 'meatballs': 251,\n",
       " '6': 252,\n",
       " 'fourteen': 253,\n",
       " 'twelve': 254,\n",
       " '1': 255,\n",
       " '4': 256,\n",
       " 'new': 257,\n",
       " 'nine': 258,\n",
       " '15': 259,\n",
       " 'fifteen': 260,\n",
       " 'combination': 261,\n",
       " '13': 262,\n",
       " '14': 263,\n",
       " 'an': 264,\n",
       " 'thirteen': 265,\n",
       " 'style': 266,\n",
       " 'every': 267,\n",
       " 'gluten': 268,\n",
       " 'free': 269,\n",
       " 'cokes': 270,\n",
       " 'lover': 271,\n",
       " 'yorker': 272,\n",
       " 'keto': 273,\n",
       " 'chicago': 274,\n",
       " 'gluten-free': 275,\n",
       " 'zero': 276,\n",
       " 'everything': 277,\n",
       " 'margarita': 278,\n",
       " 'neapolitan': 279,\n",
       " 'cauliflower': 280,\n",
       " 'topping': 281,\n",
       " 'deepdish': 282,\n",
       " 'lovers': 283,\n",
       " 'sourdough': 284,\n",
       " 'york': 285,\n",
       " 'meatlovers': 286,\n",
       " 'veggie': 287,\n",
       " 'mediterranean': 288,\n",
       " 'toppings': 289,\n",
       " 'works': 290,\n",
       " 'margherita': 291,\n",
       " 'vegetarian': 292,\n",
       " 'pan': 293,\n",
       " 'vegetables': 294,\n",
       " 'napolitana': 295,\n",
       " 'mexican': 296,\n",
       " 'hawaiian': 297,\n",
       " 'veggies': 298,\n",
       " 'coffees': 299,\n",
       " 'coffee': 300,\n",
       " 'supreme': 301,\n",
       " 'deep': 302,\n",
       " 'dish': 303,\n",
       " 'med': 304,\n",
       " 'napolitan': 305}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 33) dtype=int32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m target_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(target_sequences, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluating the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(input_sequences, target_data)\n",
      "File \u001b[1;32mc:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerac28qxm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Yara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(16, 33) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "# Dummy target data for training\n",
    "target_data = np.expand_dims(target_sequences, -1)\n",
    "\n",
    "# Training the model\n",
    "model.fit(input_sequences, target_data, epochs=10, batch_size=16)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(input_sequences, target_data)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9997 - loss: 0.0036  \n",
      "Loss: 0.006189607549458742\n",
      "Accuracy: 0.9994775056838989\n"
     ]
    }
   ],
   "source": [
    "dev_input_texts = df_dev[\"dev.SRC\"]\n",
    "dev_target_texts = df_dev[\"dev.EXR\"]\n",
    "\n",
    "# Converting texts to sequences\n",
    "dev_input_sequences = tokenizer.texts_to_sequences(dev_input_texts)\n",
    "dev_target_sequences = tokenizer.texts_to_sequences(dev_target_texts)\n",
    "\n",
    "# Padding sequences\n",
    "dev_input_sequences = pad_sequences(dev_input_sequences, maxlen=max_input_length)\n",
    "dev_target_sequences = pad_sequences(dev_target_sequences, maxlen=max_target_length)\n",
    "\n",
    "# Dummy target data for evaluation\n",
    "dev_target_data = np.expand_dims(dev_target_sequences, -1)\n",
    "\n",
    "# Evaluating the model on dev set\n",
    "loss, accuracy = model.evaluate([dev_input_sequences, dev_target_sequences], dev_target_data)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "                                          Input Text  \\\n",
      "0  i want to order two medium pizzas with sausage...   \n",
      "1           five medium pizzas with tomatoes and ham   \n",
      "2  i need to order one large vegetarian pizza wit...   \n",
      "3   i'd like to order a large onion and pepper pizza   \n",
      "4  i'll have one pie along with pesto and ham but...   \n",
      "5  i need to order one large pizza with ham bacon...   \n",
      "6  i would like a thin crust medium pizza with tu...   \n",
      "7  i need a pizza with pesto and peppers hold the...   \n",
      "8  i wanted to have five pies with peppers pesto ...   \n",
      "9    can i get two pies with peppers and bacon pesto   \n",
      "\n",
      "                                         Target Text  \\\n",
      "0  (ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) ...   \n",
      "1  (ORDER (PIZZAORDER (NUMBER 5 ) (SIZE MEDIUM ) ...   \n",
      "2  (ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (...   \n",
      "3  (ORDER (PIZZAORDER (NUMBER 1 ) (SIZE LARGE ) (...   \n",
      "4  (ORDER (PIZZAORDER (NOT (TOPPING OLIVES ) ) (N...   \n",
      "5  (ORDER (DRINKORDER (DRINKTYPE COKE ) (NUMBER 6...   \n",
      "6  (ORDER (PIZZAORDER (NOT (TOPPING PINEAPPLE ) )...   \n",
      "7  (ORDER (PIZZAORDER (NOT (TOPPING BACON ) ) (NU...   \n",
      "8  (ORDER (PIZZAORDER (NUMBER 5 ) (TOPPING ONIONS...   \n",
      "9  (ORDER (PIZZAORDER (NUMBER 2 ) (TOPPING BACON ...   \n",
      "\n",
      "                                      Predicted Text  \n",
      "0  order pizzaorder number 2 size medium complex ...  \n",
      "1  order pizzaorder number 5 size medium topping ...  \n",
      "2  order pizzaorder number 1 size large style veg...  \n",
      "3  order pizzaorder number 1 size large topping o...  \n",
      "4  order pizzaorder not topping olives number 1 t...  \n",
      "5  order drinkorder drinktype coke number 6 size ...  \n",
      "6  order pizzaorder not topping pineapple number ...  \n",
      "7  order pizzaorder not topping bacon number 1 to...  \n",
      "8  order pizzaorder number 5 topping onions toppi...  \n",
      "9  order pizzaorder number 2 topping bacon toppin...  \n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([dev_input_sequences, dev_target_sequences])\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Decoding the sequences\n",
    "decoded_preds = []\n",
    "for pred in preds:\n",
    "    decoded_preds.append(\" \".join([tokenizer.index_word[word] for word in pred if word != 0]))\n",
    "\n",
    "# Displaying the predictions\n",
    "df_preds = pd.DataFrame(list(zip(dev_input_texts, dev_target_texts, decoded_preds)), columns=[\"Input Text\", \"Target Text\", \"Predicted Text\"])\n",
    "print(df_preds.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv(\"predictions.csv\", index=False)\n",
    "model.save(\"pointer_generator_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         56,  22,  12,  43,  35,  54,  20,   7, 182,  42,  79]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"i would like a thin crust medium pizza with tuna but no pineapple\"\n",
    "test_sequence = tokenizer.texts_to_sequences([test_sentence])\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=max_input_length)\n",
    "test_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "  Input Text                                        Target Text  \\\n",
      "0          i  (ORDER (PIZZAORDER (NUMBER 2 ) (SIZE MEDIUM ) ...   \n",
      "\n",
      "                                 Predicted Text  \n",
      "0  order thin crust can up up tuna oz pineapple  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = model.predict([test_sequence,test_sequence])\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Decoding the sequences\n",
    "decoded_preds = []\n",
    "for pred in preds:\n",
    "    decoded_preds.append(\" \".join([tokenizer.index_word[word] for word in pred if word != 0]))\n",
    "\n",
    "# Displaying the predictions\n",
    "df_preds_2 = pd.DataFrame(list(zip(test_sentence, dev_target_texts, decoded_preds)), columns=[\"Input Text\", \"Target Text\", \"Predicted Text\"])\n",
    "print(df_preds_2.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
